{"componentChunkName":"component---src-templates-tag-tsx","path":"/tag/kubernetes/","result":{"data":{"ghostTag":{"slug":"kubernetes","name":"Kubernetes","visibility":"public","feature_image":null,"featureImageSharp":null,"description":null,"meta_title":null,"meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5fc64b2616db8f0039b432b7","title":"Observing Kubernetes Ingress Traffic using Metrics","slug":"observing-kubernetes-ingress-traffic-using-metrics","featured":true,"feature_image":"https://containous.ghost.io/content/images/2020/12/Blog@2x-2.png","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/e1059f243239fbb89e6142ce8204cd42/f3583/Blog%402x-2.png","srcSet":"/static/e1059f243239fbb89e6142ce8204cd42/630fb/Blog%402x-2.png 300w,\n/static/e1059f243239fbb89e6142ce8204cd42/2a4de/Blog%402x-2.png 600w,\n/static/e1059f243239fbb89e6142ce8204cd42/f3583/Blog%402x-2.png 1200w,\n/static/e1059f243239fbb89e6142ce8204cd42/bbee5/Blog%402x-2.png 1800w,\n/static/e1059f243239fbb89e6142ce8204cd42/0ef64/Blog%402x-2.png 2400w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"Monitoring Kubernetes ingress traffic is a critical part of an effective strategy for detecting and managing potential issues in real-time.","custom_excerpt":"Monitoring Kubernetes ingress traffic is a critical part of an effective strategy for detecting and managing potential issues in real-time.","visibility":"public","created_at_pretty":"01 December, 2020","published_at_pretty":"December 3, 2020","updated_at_pretty":"08 December, 2020","created_at":"2020-12-01T13:54:46.000+00:00","published_at":"2020-12-03T02:13:39.000+00:00","updated_at":"2020-12-08T03:16:46.000+00:00","meta_title":"Observing Kubernetes Ingress Traffic using Metrics","meta_description":"Monitoring Kubernetes ingress traffic is a critical part of an effective strategy for detecting and managing potential issues in real-time.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/12/Twitter@2x-1.png","twitter_title":null,"authors":[{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"}],"primary_author":{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Enterprise engineering teams are continuously striving to deliver the best user\nfacing experience possible for the applications they manage. Adopting Kubernetes\n(k8s) is helping in this regard by allowing organizations to easily manage\nlifecycle operations for workloads in a repeatable manner. Because of this,\nKubernetes has been a key enabler towards accelerating implementation of\npurpose-specific services to meet business requirements. While k8s provides a\nstrong foundation for stable operations, proactive measures must still be taken\nto avoid negative impacts including performance or functional issues. Monitoring\nKubernetes ingress traffic is a critical part of an effective strategy for\ndetecting and managing potential issues in real-time. In this article, we’ll\ndiscuss this topic including:\n\n * Where to integrate in a Kubernetes system to obtain metrics\n * How monitoring data can be stored effectively\n * What visualization tools can be used to understand metrics data\n\nObtaining ingress traffic metrics\nWhere and how ingress traffic network statistics can be collected depends upon\nthe adopted approach\n[https://traefik.io/blog/combining-ingress-controllers-and-external-load-balancers-with-kubernetes/] \nfor exposing services. Let’s dive into the two strategies often used with\nKubernetes:\n\n 1. External Load Balancers\n 2. Ingress Controllers\n\nMetrics with External Load Balancers\nWhen service configurations are defined appropriately by operators, Kubernetes\ncan automate the deployment of managed load balancers. For example, when\nlaunched within AWS, a Kubernetes Service can instantiate a cloud load balancer\ninstance without any out-of-band provisioning steps required by developers. The\nimplementations of these load balancers are black boxes, but they all expose\nmechanisms for monitoring and tracing. In the case of AWS, time-series data and\nrequest logs can be collected and accessed using accompanying integrated\nservices such as CloudWatch and CloudTrail. One potential drawback of these\ntightly integrated proprietary services is that these systems often do not\nintegrate easily with external data storage and visualization tools.\n\nCapturing Metrics from Load Balancers and Ingress ControllersMetrics with\nIngress Controllers\nWhen thinking about monitoring Ingress Controllers, it’s useful to keep in mind\nthat they are implemented as standard Kubernetes applications. This means that\nany monitoring approaches adopted by organizations to track the health and\nliveliness of k8s workloads can be applied to Ingress Controllers. Tracking\nnetwork traffic statistics in particular, however, requires taking advantage of\ncontroller-specific mechanisms. Similar to external load balancers, the specific\nmetrics exposed vary depending on the controller, but any production quality\nimplementation will provide built-in metric collection capabilities that can be\nintegrated with an external data storage system.\n\nData storage for Ingress Monitoring\nSelecting a data storage solution is an important part of defining a traffic\nmonitoring architecture for Ingress Controllers. There are two general\ncategories of implementations that one can choose between: \n\n 1. Self-managed (typically open source) database systems\n 2. Managed SaaS database systems.\n\nWhile applications often use general-purpose SQL databases for their structured\ndata, with monitoring data it’s advantageous to utilize a system optimized for\nstoring and querying time-series data. There are multiple open source options\nfor time-series databases, including InfluxDB [https://www.influxdata.com/] and \nPrometheus [https://prometheus.io/], either is typically deployed in the same\nk8s cluster as the Ingress controller. Once these systems are provisioned, it’s\njust a matter of configuring settings for the Ingress controller to enable\nautomated metric data collection in the database.\n\nSome teams may prefer to take the route of a fully managed data provider to\navoid the overheads of maintaining a database. There are options such as DataDog\n[https://datadoghq.com] or Elasticsearch [https://www.elastic.co/elastic-stack] \navailable that meet this demand in the form of a cloud-based SaaS monitoring\nplatform. The providers are well supported by controller implementations due to\ntheir popularity, and Ingress controllers such as Traefik are designed to easily\nintegrate with it.\n\nVisualizing Ingress Traffic Metrics\nGetting metrics into a database is a first step towards effective ingress\nmonitoring, but providing engineering teams with actionable information in\nreal-time requires being able to easily interpret the information. Visualizing\ntime-series data is the most effective way to convert raw metrics into\nhuman-digestible form. For example, visualizations are often combined into\ndashboards and used by Site Reliability Engineers to track the status of\nservices and as an information source when live-site issues occur. Grafana\n[https://grafana.com/] is a widely adopted open source software for data\nvisualization which works well with databases such as InfluxDB and Prometheus.\nAlternatively, the managed solutions mentioned earlier, such as DataDog, provide\nbuilt-in visualization capabilities as part of their holistic platforms.\n\n\n\n\n\nSummary\nMonitoring ingress traffic is an important part of managing the health of\nexternal-facing services. As outlined in this article, best-of-breed metric\nstorage and visualization technologies can be adopted for Kubernetes monitoring\nby virtue of capabilities provided by Ingress Controller implementations such as\nTraefik. This means organizations can easily rollout effective monitoring for\nk8s clusters providing peace of mind for engineering teams and end-users.\n\nFurthermore, Traefik metrics and reporting are available through Traefik\nEnterprise and Traefik Pilot. You can sign up for a demo\n[https://info.traefik.io/en/request-demo-traefik-enterprise] of Traefik\nEnterprise today, and Traefik Pilot [https://traefik.io/traefik-pilot/] is\nalready available for users of the popular open source Traefik Proxy project.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/12/Blog@2x.png\" class=\"kg-image\" alt=\"Observing Kubernetes Ingress Traffic using Metrics\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/12/Blog@2x.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/12/Blog@2x.png 1000w, https://containous.ghost.io/content/images/size/w1600/2020/12/Blog@2x.png 1600w, https://containous.ghost.io/content/images/2020/12/Blog@2x.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Enterprise engineering teams are continuously striving to deliver the best user facing experience possible for the applications they manage. Adopting Kubernetes (k8s) is helping in this regard by allowing organizations to easily manage lifecycle operations for workloads in a repeatable manner. Because of this, Kubernetes has been a key enabler towards accelerating implementation of purpose-specific services to meet business requirements. While k8s provides a strong foundation for stable operations, proactive measures must still be taken to avoid negative impacts including performance or functional issues. Monitoring Kubernetes ingress traffic is a critical part of an effective strategy for detecting and managing potential issues in real-time. In this article, we’ll discuss this topic including:</p><ul><li>Where to integrate in a Kubernetes system to obtain metrics</li><li>How monitoring data can be stored effectively</li><li>What visualization tools can be used to understand metrics data</li></ul><h3 id=\"obtaining-ingress-traffic-metrics\">Obtaining ingress traffic metrics</h3><p>Where and how ingress traffic network statistics can be collected depends upon the <a href=\"https://traefik.io/blog/combining-ingress-controllers-and-external-load-balancers-with-kubernetes/\">adopted approach</a> for exposing services. Let’s dive into the two strategies often used with Kubernetes:</p><ol><li>External Load Balancers</li><li>Ingress Controllers</li></ol><!--kg-card-begin: markdown--><h3 id=\"metricswithexternalloadbalancers\">Metrics with External Load Balancers</h3>\n<p>When service configurations are defined appropriately by operators, Kubernetes can automate the deployment of managed load balancers. For example, when launched within AWS, a Kubernetes Service can instantiate a cloud load balancer instance without any out-of-band provisioning steps required by developers. The implementations of these load balancers are black boxes, but they all expose mechanisms for monitoring and tracing. In the case of AWS, time-series data and request logs can be collected and accessed using accompanying integrated services such as CloudWatch and CloudTrail. One potential drawback of these tightly integrated proprietary services is that these systems often do not integrate easily with external data storage and visualization tools.</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/12/Capturing-Metrics-from-Load-Balancers-and-Ingress-Controllers.png\" class=\"kg-image\" alt=\"Capturing Metrics from Load Balancers and Ingress Controllers\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/12/Capturing-Metrics-from-Load-Balancers-and-Ingress-Controllers.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/12/Capturing-Metrics-from-Load-Balancers-and-Ingress-Controllers.png 1000w, https://containous.ghost.io/content/images/size/w1600/2020/12/Capturing-Metrics-from-Load-Balancers-and-Ingress-Controllers.png 1600w, https://containous.ghost.io/content/images/2020/12/Capturing-Metrics-from-Load-Balancers-and-Ingress-Controllers.png 2310w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Capturing Metrics from Load Balancers and Ingress Controllers</figcaption></figure><!--kg-card-begin: markdown--><h3 id=\"metricswithingresscontrollers\">Metrics with Ingress Controllers</h3>\n<p>When thinking about monitoring Ingress Controllers, it’s useful to keep in mind that they are implemented as standard Kubernetes applications. This means that any monitoring approaches adopted by organizations to track the health and liveliness of k8s workloads can be applied to Ingress Controllers. Tracking network traffic statistics in particular, however, requires taking advantage of controller-specific mechanisms. Similar to external load balancers, the specific metrics exposed vary depending on the controller, but any production quality implementation will provide built-in metric collection capabilities that can be integrated with an external data storage system.</p>\n<!--kg-card-end: markdown--><h3 id=\"data-storage-for-ingress-monitoring\">Data storage for Ingress Monitoring</h3><p>Selecting a data storage solution is an important part of defining a traffic monitoring architecture for Ingress Controllers. There are two general categories of implementations that one can choose between: </p><ol><li>Self-managed (typically open source) database systems</li><li>Managed SaaS database systems.</li></ol><!--kg-card-begin: markdown--><p>While applications often use general-purpose SQL databases for their structured data, with monitoring data it’s advantageous to utilize a system optimized for storing and querying time-series data. There are multiple open source options for time-series databases, including <a href=\"https://www.influxdata.com/\" rel=\"nofollow\">InfluxDB</a> and <a href=\"https://prometheus.io/\" rel=\"nofollow\">Prometheus</a>, either is typically deployed in the same k8s cluster as the Ingress controller. Once these systems are provisioned, it’s just a matter of configuring settings for the Ingress controller to enable automated metric data collection in the database.</p>\n<p>Some teams may prefer to take the route of a fully managed data provider to avoid the overheads of maintaining a database. There are options such as <a href=\"https://datadoghq.com\" rel=\"nofollow\">DataDog</a> or <a href=\"https://www.elastic.co/elastic-stack\" rel=\"nofollow\">Elasticsearch</a> available that meet this demand in the form of a cloud-based SaaS monitoring platform. The providers are well supported by controller implementations due to their popularity, and Ingress controllers such as Traefik are designed to easily integrate with it.</p>\n<h2 id=\"visualizingingresstrafficmetrics\">Visualizing Ingress Traffic Metrics</h2>\n<p>Getting metrics into a database is a first step towards effective ingress monitoring, but providing engineering teams with actionable information in real-time requires being able to easily interpret the information. Visualizing time-series data is the most effective way to convert raw metrics into human-digestible form. For example, visualizations are often combined into dashboards and used by Site Reliability Engineers to track the status of services and as an information source when live-site issues occur. <a href=\"https://grafana.com/\" rel=\"nofollow\">Grafana</a> is a widely adopted open source software for data visualization which works well with databases such as InfluxDB and Prometheus. Alternatively, the managed solutions mentioned earlier, such as DataDog, provide built-in visualization capabilities as part of their holistic platforms.</p>\n<p><img src=\"https://containous.ghost.io/content/images/2020/12/2020-12-01_8-53-38.png\" alt=\"Traefik Metrics\"></p>\n<p><img src=\"https://containous.ghost.io/content/images/2020/12/2020-12-01_8-16-14.png\" alt=\"Load Balancer Metrics\"></p>\n<h2 id=\"summary\">Summary</h2>\n<p>Monitoring ingress traffic is an important part of managing the health of external-facing services. As outlined in this article, best-of-breed metric storage and visualization technologies can be adopted for Kubernetes monitoring by virtue of capabilities provided by Ingress Controller implementations such as Traefik. This means organizations can easily rollout effective monitoring for k8s clusters providing peace of mind for engineering teams and end-users.</p>\n<p>Furthermore, Traefik metrics and reporting are available through Traefik Enterprise and Traefik Pilot. You can <a href=\"https://info.traefik.io/en/request-demo-traefik-enterprise\">sign up for a demo</a> of Traefik Enterprise today, and <a href=\"https://traefik.io/traefik-pilot/\">Traefik Pilot</a> is already available for users of the popular open source Traefik Proxy project.</p>\n<!--kg-card-end: markdown-->","url":"https://containous.ghost.io/blog/observing-kubernetes-ingress-traffic-using-metrics/","canonical_url":null,"uuid":"90c78d85-ea40-4c48-b992-5eb268482ba7","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5fc64b2616db8f0039b432b7","reading_time":4}},{"node":{"id":"Ghost__Post__5f7f21459ccef80039928ddc","title":"Leveraging your Ingress Controller to easily migrate to Kubernetes","slug":"leveraging-your-ingress-controller-to-easily-migrate-to-kubernetes","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/ea29e913bf4a4db59cb25046b9525998/47498/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg","srcSet":"/static/ea29e913bf4a4db59cb25046b9525998/9dc27/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg 300w,\n/static/ea29e913bf4a4db59cb25046b9525998/4fe8c/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg 600w,\n/static/ea29e913bf4a4db59cb25046b9525998/47498/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg 1200w,\n/static/ea29e913bf4a4db59cb25046b9525998/52258/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg 1800w,\n/static/ea29e913bf4a4db59cb25046b9525998/a41d1/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes-1.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"In this article, we’ll delve into the question of migrating legacy applications by discussing the specific challenges these workloads pose and outlining a strategy to overcome them.","custom_excerpt":"In this article, we’ll delve into the question of migrating legacy applications by discussing the specific challenges these workloads pose and outlining a strategy to overcome them.","visibility":"public","created_at_pretty":"08 October, 2020","published_at_pretty":"November 10, 2020","updated_at_pretty":"03 December, 2020","created_at":"2020-10-08T14:25:09.000+00:00","published_at":"2020-11-10T01:46:18.000+00:00","updated_at":"2020-12-03T02:14:07.000+00:00","meta_title":"Leveraging your Ingress Controller to easily migrate to Kubernetes","meta_description":"This article delves into migrating legacy applications by discussing the specific challenges these workloads pose and outlining a strategy to overcome them.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes---Twitter.png","twitter_title":null,"authors":[{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"}],"primary_author":{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Many enterprise organizations choose Kubernetes (k8s) [https://kubernetes.io] as\nthe foundation of their IT modernization efforts due to its alignment with cloud\nnative practices. However, a question naturally arises during the adoption\nprocess: How should existing legacy applications be handled as part of a broader\nKubernetes migration? As it turns out, the answer ties to the Ingress\nController, one of the core components in a Kubernetes cluster. In this article,\nwe’ll delve into the question of migrating legacy applications by discussing the\nspecific challenges these workloads pose and outlining a strategy to overcome\nthem.\n\nLegacy applications and Kubernetes\nThe functionality provided by legacy workloads typically encapsulates\nsignificant business value for organizations. Having been designed and\nimplemented in an earlier era, they also tend to gravitate towards monolithic\narchitectures powered by older programming languages and toolchains. For these\nreasons, they're often stagnant with little ongoing development other than to\naddress high priority bugs or significant security vulnerabilities.\n\nHere lies the dilemma: on the one hand, these legacy workloads are highly\nvaluable. Modifying them in any way, including their operating environment,\ncreates risks associated with the business's daily operation. On the other hand,\nleaving them out of a Kubernetes migration means maintaining older operating\nenvironments and preventing teams from reaping the benefits provided by\nKubernetes with these critical applications. What's needed is a way for IT\nleaders to mitigate the migration risks associated with legacy applications.\n\nDiscussions around Ingress Controllers often arise as part of networking and\nrouting in Kubernetes, particularly in connecting external users to applications\n[https://traefik.io/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/]\n. Due to their strategic placement in the overall architecture, in practice,\ntheir capabilities can extend benefits to use cases well beyond just\nconnectivity. As we'll discuss in more detail, used effectively, Ingress\nControllers can help ease the process of migrating and running legacy\napplications on Kubernetes by reducing or mitigating many of the risks that may\notherwise prevent IT from making the transition process. To illustrate where\nIngress Controllers fit into the overall migration picture, consider a\nhigh-level outline of a general migration strategy (we'll dive into each area\nnext):\n\nMigrating your Legacy Applications using Kubernetes Ingress * Deploy legacy\n   workloads on Kubernetes - Get legacy applications running on k8s as simply\n   and quickly as possible\n * Select an option\n   for ongoing development / maintenance: * Build around a legacy application -\n      Use Ingress Controller functionality to route traffic in a manner which\n      allows for building on top of legacy without modifying it\n    * Build your way out of a legacy\n      application - Use Ingress Controller functionality to enable iterative\n      refactoring of legacy\n   \n   \n\nLift and Shift: Deploy legacy workloads on Kubernetes\nThe first step of our migration strategy entails establishing a baseline for\ndeploying legacy codebase running on Kubernetes. The goal is to help achieve a\nstandardizing operating environment for all workloads and serve as a starting\npoint for further improvements. To accomplish this, one must containerize the\nmonolithic codebase and its associated dependencies. While there is no single\nrecipe for containerization that will work across all applications, there are\nwell-known items that need addressing as part of a \"lift and shift\" operation.\n\nFirst, an appropriate Docker base image should be selected or defined for the\nlegacy application. Depending upon the language and technology stack used in its\nimplementation, there may be viable candidates available on Docker Hub\n[https://hub.docker.com/]. Otherwise, DevOps engineers will need to craft a\ncustom image. Once the team establishes a base image, they leverage it to\niterate on the monolithic application's candidate release images. In some cases,\nengineers will augment base images by injecting build artifacts. In others, it\nmay be necessary to generate artifacts using the base image itself through\nmulti-stage build processes. Once the containerized image is available, it can\nbe deployed onto a Kubernetes cluster and validated.\n\nBuild around legacy applications with Ingress Controllers\nOnce the team establishes a baseline deployment, they have options for managing\nthe future legacy workload. There will inevitably be a need to extend the\nmonolith functionally, and this is where Ingress Controllers can help reduce\ncomplexity and risk. Specifically, instead of taking an approach where\ndevelopers must modify or refactor the legacy applications, the core application\ncan be left intact while using Ingress Controllers. This approach permits\nadditional functionality by injecting new services that logically sit between\nend users and the monolith. Since developers are empowered to build these\nservices from scratch, they are implementable using cloud native best practices.\nTraffic from external users routes to the intervening service layer by\nconfiguring the Ingress Controller for the cluster. When requests are received,\nthe containerized legacy application operates as needed for specific\nfunctionality.\n\nBuild away from legacy applications with Ingress Controllers\nAn alternative approach towards realizing additional functionality around a\nlegacy application once deployed on Kubernetes is to employ the so-called\nStrangler pattern. As may be apparent from the name, this strategy consists of\nreplacing legacy codebases gradually by migrating features to new microservice\nimplementations, which may also incorporate additional capabilities. Compared to\na wholesale reimplementation, the overall risk spreads over time. In addition,\nif needed, teams can always fall back to the original implementation since it is\nleft intact. The Ingress Controller is the key to enabling this strategy on\nKubernetes as it allows operators to route traffic from external users to the\nrefactored microservices versus the legacy application. As functionality\ncontinues to shift away from the monolith, it is \"strangled\" out, and\neventually, the legacy application is ready to be removed from the cluster\naltogether.\n\nConclusion\nFor many enterprise organizations, legacy applications continue to support\ncritical processes that form the business's backbone. Therefore, IT leaders need\nto understand potential strategies for handling these workloads during a\nKubernetes migration.\n\nIn this article, we've reviewed how Ingress Controllers can significantly reduce\nthe risk of legacy migrations while also enabling continued development around\nlegacy implementations. While the directions outlined are available today with\navailable Ingress Controllers, this area is also rapidly evolving within the\nKubernetes ecosystem, as evidenced by its Service API evolution\n[https://traefik.io/blog/kubernetes-ingress-service-api-demystified/].\nEnterprises can safely assume that the ability to leverage resources such as\nIngress Controllers to help ease migration challenges is only going to improve\nin the future.\n\nLearn more about Traefik Enterprise\n[https://info.traefik.io/en/request-demo-traefik-enterprise] today and learn how\nbusinesses are leveraging the power of enterprise-grade Kubernetes Ingress to\nsolve their most demanding challenges.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes.jpg\" class=\"kg-image\" alt=\"Leveraging your Ingress Controller to easily migrate to Kubernetes\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes.jpg 1000w, https://containous.ghost.io/content/images/size/w1600/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes.jpg 1600w, https://containous.ghost.io/content/images/2020/11/Leveraging-your-Ingress-Controller-to-easily-migrate-to-Kubernetes.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: markdown--><p>Many enterprise organizations choose <a href=\"https://kubernetes.io\" target=\"_blank\" rel=\"nofollow\">Kubernetes (k8s)</a> as the foundation of their IT modernization efforts due to its alignment with cloud native practices. However, a question naturally arises during the adoption process: How should existing legacy applications be handled as part of a broader Kubernetes migration? As it turns out, the answer ties to the Ingress Controller, one of the core components in a Kubernetes cluster. In this article, we’ll delve into the question of migrating legacy applications by discussing the specific challenges these workloads pose and outlining a strategy to overcome them.</p>\n<h2 id=\"legacyapplicationsandkubernetes\">Legacy applications and Kubernetes</h2>\n<p>The functionality provided by legacy workloads typically encapsulates significant business value for organizations. Having been designed and implemented in an earlier era, they also tend to gravitate towards monolithic architectures powered by older programming languages and toolchains. For these reasons, they're often stagnant with little ongoing development other than to address high priority bugs or significant security vulnerabilities.</p>\n<p>Here lies the dilemma: on the one hand, these legacy workloads are highly valuable. Modifying them in any way, including their operating environment, creates risks associated with the business's daily operation. On the other hand, leaving them out of a Kubernetes migration means maintaining older operating environments and preventing teams from reaping the benefits provided by Kubernetes with these critical applications. What's needed is a way for IT leaders to mitigate the migration risks associated with legacy applications.</p>\n<p>Discussions around Ingress Controllers often arise as part of networking and routing in Kubernetes, particularly in <a href=\"https://traefik.io/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/\">connecting external users to applications</a>. Due to their strategic placement in the overall architecture, in practice, their capabilities can extend benefits to use cases well beyond just connectivity. As we'll discuss in more detail, used effectively, Ingress Controllers can help ease the process of migrating and running legacy applications on Kubernetes by reducing or mitigating many of the risks that may otherwise prevent IT from making the transition process. To illustrate where Ingress Controllers fit into the overall migration picture, consider a high-level outline of a general migration strategy (we'll dive into each area next):</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/10/Strangler_Pattern_Blog_Image.svg\" class=\"kg-image\" alt=\"Migrating your Legacy Applications using Kubernetes Ingress\"><figcaption>Migrating your Legacy Applications using Kubernetes Ingress</figcaption></figure><!--kg-card-begin: markdown--><ul>\n<li>Deploy legacy workloads on Kubernetes - Get legacy applications running on k8s as simply and quickly as possible</li>\n<li>Select an option for ongoing development / maintenance:\n<ul>\n<li>Build around a legacy application - Use Ingress Controller functionality to route traffic in a manner which allows for building on top of legacy without modifying it</li>\n<li>Build your way out of a legacy application - Use Ingress Controller functionality to enable iterative refactoring of legacy</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"liftandshiftdeploylegacyworkloadsonkubernetes\">Lift and Shift: Deploy legacy workloads on Kubernetes</h2>\n<p>The first step of our migration strategy entails establishing a baseline for deploying legacy codebase running on Kubernetes. The goal is to help achieve a standardizing operating environment for all workloads and serve as a starting point for further improvements. To accomplish this, one must containerize the monolithic codebase and its associated dependencies. While there is no single recipe for containerization that will work across all applications, there are well-known items that need addressing as part of a &quot;lift and shift&quot; operation.</p>\n<p>First, an appropriate Docker base image should be selected or defined for the legacy application. Depending upon the language and technology stack used in its implementation, there may be viable candidates available on <a href=\"https://hub.docker.com/\" rel=\"nofollow\" target=\"_blank\">Docker Hub</a>. Otherwise, DevOps engineers will need to craft a custom image. Once the team establishes a base image, they leverage it to iterate on the monolithic application's candidate release images. In some cases, engineers will augment base images by injecting build artifacts. In others, it may be necessary to generate artifacts using the base image itself through multi-stage build processes. Once the containerized image is available, it can be deployed onto a Kubernetes cluster and validated.</p>\n<h2 id=\"buildaroundlegacyapplicationswithingresscontrollers\">Build around legacy applications with Ingress Controllers</h2>\n<p>Once the team establishes a baseline deployment, they have options for managing the future legacy workload. There will inevitably be a need to extend the monolith functionally, and this is where Ingress Controllers can help reduce complexity and risk. Specifically, instead of taking an approach where developers must modify or refactor the legacy applications, the core application can be left intact while using Ingress Controllers. This approach permits additional functionality by injecting new services that logically sit between end users and the monolith. Since developers are empowered to build these services from scratch, they are implementable using cloud native best practices. Traffic from external users routes to the intervening service layer by configuring the Ingress Controller for the cluster. When requests are received, the containerized legacy application operates as needed for specific functionality.</p>\n<h2 id=\"buildawayfromlegacyapplicationswithingresscontrollers\">Build away from legacy applications with Ingress Controllers</h2>\n<p>An alternative approach towards realizing additional functionality around a legacy application once deployed on Kubernetes is to employ the so-called Strangler pattern. As may be apparent from the name, this strategy consists of replacing legacy codebases gradually by migrating features to new microservice implementations, which may also incorporate additional capabilities. Compared to a wholesale reimplementation, the overall risk spreads over time. In addition, if needed, teams can always fall back to the original implementation since it is left intact. The Ingress Controller is the key to enabling this strategy on Kubernetes as it allows operators to route traffic from external users to the refactored microservices versus the legacy application. As functionality continues to shift away from the monolith, it is &quot;strangled&quot; out, and eventually, the legacy application is ready to be removed from the cluster altogether.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>For many enterprise organizations, legacy applications continue to support critical processes that form the business's backbone. Therefore, IT leaders need to understand potential strategies for handling these workloads during a Kubernetes migration.</p>\n<p>In this article, we've reviewed how Ingress Controllers can significantly reduce the risk of legacy migrations while also enabling continued development around legacy implementations. While the directions outlined are available today with available Ingress Controllers, this area is also rapidly evolving within the Kubernetes ecosystem, as evidenced by its <a href=\"https://traefik.io/blog/kubernetes-ingress-service-api-demystified/\">Service API evolution</a>. Enterprises can safely assume that the ability to leverage resources such as Ingress Controllers to help ease migration challenges is only going to improve in the future.</p>\n<p><a href=\"https://info.traefik.io/en/request-demo-traefik-enterprise\">Learn more about Traefik Enterprise</a> today and learn how businesses are leveraging the power of enterprise-grade Kubernetes Ingress to solve their most demanding challenges.</p>\n<!--kg-card-end: markdown-->","url":"https://containous.ghost.io/blog/leveraging-your-ingress-controller-to-easily-migrate-to-kubernetes/","canonical_url":null,"uuid":"0428ce65-eac9-4f98-9d45-20c721ff9acb","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5f7f21459ccef80039928ddc","reading_time":4}},{"node":{"id":"Ghost__Post__5f5970b6ff947b00398fd523","title":"Achieve zero-downtime deployments with Traefik and Kubernetes","slug":"achieve-zero-downtime-deployments-with-traefik-and-kubernetes","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/bb8460cc002da7893c5dd6af9f4fc7a0/47498/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg","srcSet":"/static/bb8460cc002da7893c5dd6af9f4fc7a0/9dc27/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg 300w,\n/static/bb8460cc002da7893c5dd6af9f4fc7a0/4fe8c/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg 600w,\n/static/bb8460cc002da7893c5dd6af9f4fc7a0/47498/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg 1200w,\n/static/bb8460cc002da7893c5dd6af9f4fc7a0/52258/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg 1800w,\n/static/bb8460cc002da7893c5dd6af9f4fc7a0/a41d1/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes-1.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"Containers and Kubernetes have revolutionized software delivery. Releasing apps and services as stateless container images makes it easy to create and destroy container instances as demand requires.","custom_excerpt":"Containers and Kubernetes have revolutionized software delivery. Releasing apps and services as stateless container images makes it easy to create and destroy container instances as demand requires.","visibility":"public","created_at_pretty":"10 September, 2020","published_at_pretty":"September 10, 2020","updated_at_pretty":"12 October, 2020","created_at":"2020-09-10T00:17:58.000+00:00","published_at":"2020-09-10T14:18:42.000+00:00","updated_at":"2020-10-12T23:57:15.000+00:00","meta_title":"Achieve zero-downtime deployments with Traefik and Kubernetes","meta_description":"Blue-green deployments, canary releases, and A/B testing are three popular network-centric approaches to software testing and deployment.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes---Twitter.png","twitter_title":null,"authors":[{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Rapid release cycles are the hallmark of modern software development. The\ncloud-native ecosystem tooling includes containers, Kubernetes, microservices,\nand agile development methods, all of which support and encourage frequent\ndelivery and deployment. The hidden downside, however, is that every change to\nproduction is another chance for something to go wrong.\n\nIn the worst-case scenario, that \"something\" means downtime.\n\nTesting is always the first line of defense against service disruption.\nUnfortunately, cloud-native application architectures can be surprisingly\ncomplex, and the full extent of the interactions between APIs and services can\nbe hard to map and predict. Because of this, the traditional testing march from\ndevelopment through staging is unlikely to catch every issue before new code\nreaches production.\n\nToday’s network-centric applications call for new, network-centric approaches to\nsoftware testing and deployment. As a modern, cloud-native edge router, Traefik\nis ready to help.\n\nRouting around downtime\nContainers and Kubernetes have revolutionized software delivery. Releasing apps\nand services as stateless container images makes it easy to create and destroy\ncontainer instances as demand requires. Is an application experiencing a traffic\nspike? Operators may add additional instances to the cluster and use a load\nbalancer to distribute requests accordingly.\n\nThis model gets really interesting, however, when instances of more than one\nversion of the same software serve requests within the same cluster. Mixing old\nand new versions like this makes it possible to configure routing rules to test\nthe production environment's latest version. More importantly, the new version\ncan be released gradually – and even withdrawn, should problems arise – all with\nvirtually no downtime.\n\nThree popular variations on this idea are blue-green deployments, canary\nreleases, and A/B testing. Although all three are related, each is also\ndistinct.\n\nBlue-green deployments\nIn this pattern, “green” refers to the current, stable version of the software,\nwhile “blue” refers to an upcoming release that introduces new features and\nfixes. Instances of both versions operate simultaneously in the same production\nenvironment. Concurrently, a proxy router (such as Traefik) ensures that only\nrequests sent to a private address can reach the blue instances.\n\nThere are two ways to test such a setup. The first is to run synthetic tests\nagainst the blue instances, confident that they are being staged in an\nenvironment that matches production exactly. A more ambitious method involves \ntraffic mirroring, in which the green instances handle incoming requests, but a\nduplicate of every request is also sent to the blue instances. Although this can\nbe resource intensive, it creates an accurate simulation of what would happen if\nthe blue instances were running the show.\n\nOnce all test cases and integrations are satisfied, switching over from the\ngreen to the blue version is as simple as updating the routing rules. In effect,\nblue becomes green, and eventually the next iteration of the software is\ndeployed as the new blue. Equally important, it’s just as easy to revert the\nrouting rules and roll back to the earlier green version, should some\nlast-minute catastrophe occur.\n\nCanary releases\nThe canary release model takes blue-green testing a step further by deploying\nnew features and patches into active production, albeit in a measured way. The\nrouter is configured such that the current, stable version of the software\nhandles most requests, but a limited percentage of requests route to instances\nof the new, “canary” version.\n\nTrue to the “canary in a coal mine” metaphor, if instances of the canary release\nstart dropping dead (or exhibiting problems in some way), they can be withdrawn\nfor bug fixes while the stable release carries on as before. If things go\nsmoothly, on the other hand, the proportion of requests handled by the canary\nrelease can be gradually increased until it reaches 100 percent.\n\nThis model breaks down quickly, however, when the canary release is too large\nand introduces too many changes at once. It works best for microservice\narchitectures, where features or fixes can be released incrementally and\nevaluated on their particular merits.\n\nA/B testing\nThis technique is sometimes confused with the previous two, but it has its own\npurpose, which is to evaluate two distinct versions of an upcoming release to\nsee which will be more successful. This tactic is common for UI development. For\nexample, suppose a new feature will soon roll out to an application, but it’s\nunclear how best to expose it to users. To find out, two versions of the UI\nincluding the the feature, run in tandem – Version A and Version B – and the\nproxy router sends a limited number of requests to each one.\n\nFrom there, metrics can help determine which version is the better choice. Does\nVersion A do a better job of convincing users to try the new feature? Do users\ncomplete the UI sequence faster using Version B, or do they tend to cancel\nbefore the end? Each version of the new UI can be trialed with a small number of\nusers while routing rules ensure that the stable version continues to serve the\nmajority of requests.\n\nNetwork effects\nThese techniques can be invaluable for testing modern, cloud-native software\narchitectures, especially when compared to traditional waterfall-style\ndeployment models. When used correctly, they can help spot unforeseen\nregressions, integration failures, performance bottlenecks, and usability issues\nwithin the production environment, but before new code graduates to a stable,\nproduction release.\n\nWhat all three approaches share in common is that they rely on the ease of\ndeployment afforded by containers and Kubernetes, coupled with cloud-native\nnetworking techniques, to route requests to testable deployments while\nminimizing disruptions to production code. That’s a powerful combination – one\nthat’s squarely within Traefik’s wheelhouse – and if employed judiciously, it\ncan effectively bring overall application downtime to zero. \n\nClick here [https://containo.us/solutions/kubernetes-ingress/] to learn more\nabout Traefik, a centralized routing solution for any Kubernetes cluster, that\nenables better application uptime.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes.jpg\" class=\"kg-image\" alt=\"Achieve zero-downtime deployments with Traefik and Kubernetes\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes.jpg 1000w, https://containous.ghost.io/content/images/size/w1600/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes.jpg 1600w, https://containous.ghost.io/content/images/2020/09/Achieve-zero-downtime-deployments-with-Traefik-and-Kubernetes.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Rapid release cycles are the hallmark of modern software development. The cloud-native ecosystem tooling includes containers, Kubernetes, microservices, and agile development methods, all of which support and encourage frequent delivery and deployment. The hidden downside, however, is that every change to production is another chance for something to go wrong.</p><p>In the worst-case scenario, that \"something\" means downtime.</p><p>Testing is always the first line of defense against service disruption. Unfortunately, cloud-native application architectures can be surprisingly complex, and the full extent of the interactions between APIs and services can be hard to map and predict. Because of this, the traditional testing march from development through staging is unlikely to catch every issue before new code reaches production.</p><p>Today’s network-centric applications call for new, network-centric approaches to software testing and deployment. As a modern, cloud-native edge router, Traefik is ready to help.</p><h2 id=\"routing-around-downtime\"><strong>Routing around downtime</strong></h2><p>Containers and Kubernetes have revolutionized software delivery. Releasing apps and services as stateless container images makes it easy to create and destroy container instances as demand requires. Is an application experiencing a traffic spike? Operators may add additional instances to the cluster and use a load balancer to distribute requests accordingly.</p><p>This model gets really interesting, however, when instances of more than one version of the same software serve requests within the same cluster. Mixing old and new versions like this makes it possible to configure routing rules to test the production environment's latest version. More importantly, the new version can be released gradually – and even withdrawn, should problems arise – all with virtually no downtime.</p><p>Three popular variations on this idea are <strong>blue-green deployments</strong>, <strong>canary releases</strong>, and <strong>A/B testing</strong>. Although all three are related, each is also distinct.</p><h3 id=\"blue-green-deployments\"><strong>Blue-green deployments</strong></h3><p>In this pattern, “green” refers to the current, stable version of the software, while “blue” refers to an upcoming release that introduces new features and fixes. Instances of both versions operate simultaneously in the same production environment. Concurrently, a proxy router (such as Traefik) ensures that only requests sent to a private address can reach the blue instances.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/09/Diagram-Blue-green-deployments.png\" class=\"kg-image\" alt=\"Blue-green deployment\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/09/Diagram-Blue-green-deployments.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/09/Diagram-Blue-green-deployments.png 1000w, https://containous.ghost.io/content/images/size/w1600/2020/09/Diagram-Blue-green-deployments.png 1600w, https://containous.ghost.io/content/images/2020/09/Diagram-Blue-green-deployments.png 1695w\" sizes=\"(min-width: 720px) 720px\"></figure><p>There are two ways to test such a setup. The first is to run synthetic tests against the blue instances, confident that they are being staged in an environment that matches production exactly. A more ambitious method involves <em>traffic mirroring</em>, in which the green instances handle incoming requests, but a duplicate of every request is also sent to the blue instances. Although this can be resource intensive, it creates an accurate simulation of what would happen if the blue instances were running the show.</p><p>Once all test cases and integrations are satisfied, switching over from the green to the blue version is as simple as updating the routing rules. In effect, blue becomes green, and eventually the next iteration of the software is deployed as the new blue. Equally important, it’s just as easy to revert the routing rules and roll back to the earlier green version, should some last-minute catastrophe occur.</p><h3 id=\"canary-releases\"><strong>Canary releases</strong></h3><p>The canary release model takes blue-green testing a step further by deploying new features and patches into active production, albeit in a measured way. The router is configured such that the current, stable version of the software handles most requests, but a limited percentage of requests route to instances of the new, “canary” version.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/09/Diagram-canary-releases.png\" class=\"kg-image\" alt=\"Canary release model\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/09/Diagram-canary-releases.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/09/Diagram-canary-releases.png 1000w, https://containous.ghost.io/content/images/size/w1600/2020/09/Diagram-canary-releases.png 1600w, https://containous.ghost.io/content/images/2020/09/Diagram-canary-releases.png 1695w\" sizes=\"(min-width: 720px) 720px\"></figure><p>True to the “canary in a coal mine” metaphor, if instances of the canary release start dropping dead (or exhibiting problems in some way), they can be withdrawn for bug fixes while the stable release carries on as before. If things go smoothly, on the other hand, the proportion of requests handled by the canary release can be gradually increased until it reaches 100 percent.</p><p>This model breaks down quickly, however, when the canary release is too large and introduces too many changes at once. It works best for microservice architectures, where features or fixes can be released incrementally and evaluated on their particular merits.</p><h3 id=\"a-b-testing\"><strong>A/B testing</strong></h3><p>This technique is sometimes confused with the previous two, but it has its own purpose, which is to evaluate two distinct versions of an upcoming release to see which will be more successful. This tactic is common for UI development. For example, suppose a new feature will soon roll out to an application, but it’s unclear how best to expose it to users. To find out, two versions of the UI including the the feature, run in tandem – Version A and Version B – and the proxy router sends a limited number of requests to each one.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/09/Diagram-A-B-Testing.png\" class=\"kg-image\" alt=\"A/B testing\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/09/Diagram-A-B-Testing.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/09/Diagram-A-B-Testing.png 1000w, https://containous.ghost.io/content/images/size/w1600/2020/09/Diagram-A-B-Testing.png 1600w, https://containous.ghost.io/content/images/2020/09/Diagram-A-B-Testing.png 1695w\" sizes=\"(min-width: 720px) 720px\"></figure><p>From there, metrics can help determine which version is the better choice. Does Version A do a better job of convincing users to try the new feature? Do users complete the UI sequence faster using Version B, or do they tend to cancel before the end? Each version of the new UI can be trialed with a small number of users while routing rules ensure that the stable version continues to serve the majority of requests.</p><h2 id=\"network-effects\"><strong>Network effects</strong></h2><p>These techniques can be invaluable for testing modern, cloud-native software architectures, especially when compared to traditional waterfall-style deployment models. When used correctly, they can help spot unforeseen regressions, integration failures, performance bottlenecks, and usability issues within the production environment, but before new code graduates to a stable, production release.</p><p>What all three approaches share in common is that they rely on the ease of deployment afforded by containers and Kubernetes, coupled with cloud-native networking techniques, to route requests to testable deployments while minimizing disruptions to production code. That’s a powerful combination – one that’s squarely within Traefik’s wheelhouse – and if employed judiciously, it can effectively bring overall application downtime to zero. </p><p><a href=\"https://containo.us/solutions/kubernetes-ingress/\">Click here</a> to learn more about Traefik, a centralized routing solution for any Kubernetes cluster, that enables better application uptime. </p>","url":"https://containous.ghost.io/blog/achieve-zero-downtime-deployments-with-traefik-and-kubernetes/","canonical_url":null,"uuid":"a656bf3f-d4a9-4122-9793-2487b9b1d472","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5f5970b6ff947b00398fd523","reading_time":4}},{"node":{"id":"Ghost__Post__5f161889d90a6e003946cbaa","title":"Simplified security for Kubernetes with Traefik and Let’s Encrypt","slug":"simplified-security-for-kubernetes-with-traefik-and-lets-encrypt","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/b529a4995103e2123aa83cf7cf178170/47498/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg","srcSet":"/static/b529a4995103e2123aa83cf7cf178170/9dc27/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg 300w,\n/static/b529a4995103e2123aa83cf7cf178170/4fe8c/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg 600w,\n/static/b529a4995103e2123aa83cf7cf178170/47498/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg 1200w,\n/static/b529a4995103e2123aa83cf7cf178170/52258/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg 1800w,\n/static/b529a4995103e2123aa83cf7cf178170/a41d1/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"Encryption for network security is a non-trivial matter, particularly in complex environments. Traefik and Let’s Encrypt can make the process of securing Kubernetes clusters simpler, speedier, and more resilient.","custom_excerpt":"Encryption for network security is a non-trivial matter, particularly in complex environments. Traefik and Let’s Encrypt can make the process of securing Kubernetes clusters simpler, speedier, and more resilient.","visibility":"public","created_at_pretty":"20 July, 2020","published_at_pretty":"July 21, 2020","updated_at_pretty":"11 August, 2020","created_at":"2020-07-20T22:19:53.000+00:00","published_at":"2020-07-21T13:58:29.000+00:00","updated_at":"2020-08-11T15:37:49.000+00:00","meta_title":"Simplified security for Kubernetes with Traefik & Let’s Encrypt","meta_description":"Traefik and Let’s Encrypt can make the process of securing Kubernetes clusters simpler, speedier, and more resilient.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Traefik and Let’s Encrypt can make the process of securing Kubernetes clusters simpler, speedier, and more resilient.","twitter_image":"https://containous.ghost.io/content/images/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt---Twitter.png","twitter_title":"Simplified security for Kubernetes with Traefik and Let’s Encrypt","authors":[{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Network encryption – also known as transport layer security (TLS) – is a must\nfor today’s online apps and services. Beyond the obvious need to prevent data\nbreaches and other malicious attacks, there are also regulatory concerns to\nconsider. Network encryption can be an important step in proving GDPR\n[https://gdpr.eu/] compliance, passing a financial audit, or complying with\ndomain-specific regulations such as HIPAA\n[https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html] \nor PCI DSS [https://www.pcisecuritystandards.org/], to name just a few examples.\n\nUnfortunately, getting encryption right is a non-trivial matter, and throwing\ncontainers and Kubernetes into the mix only compounds the issue. The distributed\nnature of Kubernetes environments and the often-brief lifecycles of\ncontainerized workloads both multiply the challenges of network security.\n\nStill, there’s hope. Newer tools and services – including Traefik and Let’s\nEncrypt [https://letsencrypt.org/] – can make the process of securing Kubernetes\nclusters simpler, speedier, and more resilient.\n\nLowering barriers to encryption\nHistorically, network encryption has been both laborious and costly. By some\nestimates, managing a typical certificate and private key can take four hours\nper year. That may not sound like much, but when multiplied by the requirements\nof a large infrastructure, this essential maintenance can quickly become\nburdensome, particularly given the current push by Apple and other vendors to\nlimit the validity of certificates to no more than 398 days\n[https://support.apple.com/en-us/HT211025].\n\nThe alternative, however, is even worse. Expired or invalid certificates can\nlead to outages and reduced service, resulting in untold losses in productivity\nand revenue [https://www.google.com/search?q=post-mortem+ssl+cert+expired].\n\nLet’s Encrypt was founded to address these issues. A project of the Internet\nSecurity Research Group [https://www.abetterinternet.org/], Let’s Encrypt is a\nfree, nonprofit, automated, and open certificate authority (CA), created with\nthe goal of promoting universal internet security and data privacy through\nencryption. Its sponsors include some of the largest software vendors and online\nservices in the industry.\n\nLet’s Encrypt will issue an unlimited number of domain-validated\n[https://casecurity.org/2013/08/07/what-are-the-different-types-of-ssl-certificates/] \ncertificates with 90-day validity to the owner of any domain name, free of\ncharge. As of this writing, the project has issued more than 1 billion\ncertificates and it counts some 200 million websites as its clients.\n\nAutomation: Let’s Encrypt’s secret sauce\nCost reduction is of course welcome, but the most important benefit of Let’s\nEncrypt is labor reduction via automation. For Kubernetes operators, automation\nis a way of life. Let’s Encrypt brings that spirit to the world of network\nsecurity.Central to how Let’s Encrypt works is the ACME (Automated Certificate\nManagement Environment) protocol, which was made an IETF standard\n[https://tools.ietf.org/html/rfc8555] in 2019. It defines a method whereby a\ncomputing service and a CA (typically Let’s Encrypt itself) can complete the\nprocess of requesting, issuing, and verifying certificates without manual\nintervention.\n\n7 Steps to Automated Domain Validation with Let’s EncryptThe ACME domain validation process [https://letsencrypt.org/how-it-works/] \nrelies on public key cryptography to establish trust. ACME client software\n[https://letsencrypt.org/docs/client-options/], also known as a certificate\nmanagement agent, generates a public-private key pair and presents it to a\nserver on the CA’s end, along with a domain name. The server then asks the\nclient to prove that it controls the domain in question by satisfying one or\nmore challenges [https://letsencrypt.org/docs/challenge-types/]. If the client\ncompletes the challenges and signs some temporary data (a “nonce”) with its\nprivate key, the server gives the client’s key pair its stamp of approval.\n\nOnce the client has an authorized key pair, it can use it to request, renew, or\nrevoke certificates from the CA, as long as the keys remain valid. Automating\nthese tasks can eliminate as much as 95% of the labor involved in managing TLS\nencryption.\n\nACME and Traefik\nSo how to get this level of automated certificate management for a Kubernetes\ncluster? One of the easier ways is to deploy the open source Traefik [/traefik/] \nedge router as a Kubernetes Ingress controller\n[https://docs.traefik.io/providers/kubernetes-ingress/]. Traefik includes a \nbuilt-in ACME client [https://docs.traefik.io/https/acme/], so no additional\nsoftware is needed to begin working with Let’s Encrypt.\n\nWhen used as an Ingress controller, Traefik ferries requests from the external\nnetwork to services running within the Kubernetes cluster, using routes defined\nby Kubernetes Ingress resources\n[https://kubernetes.io/docs/concepts/services-networking/ingress/]. Traefik’s\nACME client can automatically request and provision certificates for any domain\nassigned to the cluster, making it possible to add TLS-encrypted routes.\n\nTraefik aims to reduce setup times by automatically discovering the right\nconfiguration for a given infrastructure, so putting the pieces in place is\nstraightforward. Instructions on how to install Traefik into a Kubernetes\ncluster and enable Let’s Encrypt are available in the official documentation\n[https://docs.traefik.io/user-guides/crd-acme/].\n\nA secure starting point\nTraefik and Let’s Encrypt together offer to set up basic TLS routing for\nKubernetes. As mentioned earlier, however, encryption for network security is a\nnon-trivial matter, particularly in more complex environments. Just for\nstarters, you’ll need genuine domain names that point to servers that are\naccessible by the public internet for Let’s Encrypt’s domain-validation\nchallenge process to work.\n\nFurther challenges arise when you want to run more than one instance of Traefik,\nto ensure high availability. It’s currently not possible to run more than one\ninstance of the community-maintained version of Traefik on a cluster with Let’s\nEncrypt enabled, because there’s no guarantee that any request will be handled\nby the correct instance of Traefik. One way to address this is to use the\ncommercial Traefik Enterprise Edition (TraefikEE) [/traefikee/] product, which\noffers distributed Let’s Encrypt as a supported feature.\n\nOrganizations with more specific needs – such as interfacing with enterprise\nvault providers – and who aren’t afraid to take a hands-on, DIY approach may\nneed to install additional, dedicated certificate management software. One such\noption that’s proven popular is cert-manager [https://cert-manager.io/docs/],\nwhich can work in tandem with both Traefik and TraefikEE.\n\nThe key point, however, is that TLS encryption should be everywhere, and today\nit can be. The ACME protocol, Let’s Encrypt, and self-configuring software like\nTraefik can automate away much of the hard work that was once involved with\ncertificate management and secure request routing. In turn, that frees you to\nconcentrate on what really matters, which is how to deliver the most value with\nyour services, rather than how to secure them.\n\nYou can try out the features of TraefikEE, including distributed Let’s Encrypt,\nby starting a 30-day free trial\n[https://info.containo.us/get-traefik-enterprise-edition-free-for-30-days].","html":"<figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://containous.ghost.io/content/images/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt-1.jpg\" class=\"kg-image\" alt=\"Simplified security for Kubernetes with Traefik and Let's Encrypt\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt-1.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt-1.jpg 1000w, https://containous.ghost.io/content/images/size/w1600/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt-1.jpg 1600w, https://containous.ghost.io/content/images/2020/07/Simplified-security-for-Kubernetes-with-Traefik-and-Lets-Encrypt-1.jpg 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure><!--kg-card-begin: markdown--><p>Network encryption – also known as transport layer security (TLS) – is a must for today’s online apps and services. Beyond the obvious need to prevent data breaches and other malicious attacks, there are also regulatory concerns to consider. Network encryption can be an important step in proving <a href=\"https://gdpr.eu/\" target=\"_blank\" rel=\"nofollow\">GDPR</a> compliance, passing a financial audit, or complying with domain-specific regulations such as <a href=\"https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html\" target=\"_blank\" rel=\"nofollow\">HIPAA</a> or <a href=\"https://www.pcisecuritystandards.org/\" target=\"_blank\" rel=\"nofollow\">PCI DSS</a>, to name just a few examples.</p>\n<!--kg-card-end: markdown--><p>Unfortunately, getting encryption right is a non-trivial matter, and throwing containers and Kubernetes into the mix only compounds the issue. The distributed nature of Kubernetes environments and the often-brief lifecycles of containerized workloads both multiply the challenges of network security.</p><!--kg-card-begin: markdown--><p>Still, there’s hope. Newer tools and services – including Traefik and <a href=\"https://letsencrypt.org/\" target=\"_blank\" rel=\"nofollow\">Let’s Encrypt</a> – can make the process of securing Kubernetes clusters simpler, speedier, and more resilient.</p>\n<!--kg-card-end: markdown--><h2 id=\"lowering-barriers-to-encryption\"><strong>Lowering barriers to encryption</strong></h2><!--kg-card-begin: markdown--><p>Historically, network encryption has been both laborious and costly. By some estimates, managing a typical certificate and private key can take four hours per year. That may not sound like much, but when multiplied by the requirements of a large infrastructure, this essential maintenance can quickly become burdensome, particularly given the current push by Apple and other vendors to limit the validity of certificates to <a href=\"https://support.apple.com/en-us/HT211025\" target=\"_blank\" rel=\"nofollow\">no more than 398 days</a>.</p>\n<p>The alternative, however, is even worse. Expired or invalid certificates can lead to outages and reduced service, resulting in untold <a href=\"https://www.google.com/search?q=post-mortem+ssl+cert+expired\" target=\"_blank\" rel=\"nofollow\">losses in productivity and revenue</a>.</p>\n<p>Let’s Encrypt was founded to address these issues. A project of the <a href=\"https://www.abetterinternet.org/\" target=\"_blank\" rel=\"nofollow\">Internet Security Research Group</a>, Let’s Encrypt is a free, nonprofit, automated, and open certificate authority (CA), created with the goal of promoting universal internet security and data privacy through encryption. Its sponsors include some of the largest software vendors and online services in the industry.</p>\n<p>Let’s Encrypt will issue an unlimited number of <a href=\"https://casecurity.org/2013/08/07/what-are-the-different-types-of-ssl-certificates/\" target=\"_blank\" rel=\"nofollow\">domain-validated</a> certificates with 90-day validity to the owner of any domain name, free of charge. As of this writing, the project has issued more than 1 billion certificates and it counts some 200 million websites as its clients.</p>\n<!--kg-card-end: markdown--><h2 id=\"automation-let-s-encrypt-s-secret-sauce\"><strong>Automation: Let’s Encrypt’s secret sauce</strong></h2><!--kg-card-begin: markdown--><p>Cost reduction is of course welcome, but the most important benefit of Let’s Encrypt is labor reduction via automation. For Kubernetes operators, automation is a way of life. Let’s Encrypt brings that spirit to the world of network security.Central to how Let’s Encrypt works is the ACME (Automated Certificate Management Environment) protocol, which was made an <a href=\"https://tools.ietf.org/html/rfc8555\" target=\"_blank\" rel=\"nofollow\">IETF standard</a> in 2019. It defines a method whereby a computing service and a CA (typically Let’s Encrypt itself) can complete the process of requesting, issuing, and verifying certificates without manual intervention.</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/07/7-Steps-to-Automated-Domain-Validation-with-Lets-Encrypt.png\" class=\"kg-image\" alt=\"7 Steps to Automated Domain Validation with Let’s Encrypt\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/07/7-Steps-to-Automated-Domain-Validation-with-Lets-Encrypt.png 600w, https://containous.ghost.io/content/images/size/w1000/2020/07/7-Steps-to-Automated-Domain-Validation-with-Lets-Encrypt.png 1000w, https://containous.ghost.io/content/images/2020/07/7-Steps-to-Automated-Domain-Validation-with-Lets-Encrypt.png 1050w\" sizes=\"(min-width: 720px) 720px\"><figcaption><strong>7 Steps to Automated Domain Validation with Let’s Encrypt</strong></figcaption></figure><!--kg-card-begin: markdown--><p>The ACME <a href=\"https://letsencrypt.org/how-it-works/\" target=\"_blank\" rel=\"nofollow\">domain validation process</a> relies on public key cryptography to establish trust. <a href=\"https://letsencrypt.org/docs/client-options/\" target=\"_blank\" rel=\"nofollow\">ACME client software</a>, also known as a certificate management agent, generates a public-private key pair and presents it to a server on the CA’s end, along with a domain name. The server then asks the client to prove that it controls the domain in question by satisfying <a href=\"https://letsencrypt.org/docs/challenge-types/\" target=\"_blank\" rel=\"nofollow\">one or more challenges</a>. If the client completes the challenges and signs some temporary data (a “nonce”) with its private key, the server gives the client’s key pair its stamp of approval.</p>\n<!--kg-card-end: markdown--><p>Once the client has an authorized key pair, it can use it to request, renew, or revoke certificates from the CA, as long as the keys remain valid. Automating these tasks can eliminate as much as 95% of the labor involved in managing TLS encryption.</p><h2 id=\"acme-and-traefik\"><strong>ACME and Traefik</strong></h2><p>So how to get this level of automated certificate management for a Kubernetes cluster? One of the easier ways is to deploy the open source<a href=\"https://containous.ghost.io/traefik/\"> Traefik</a> edge router as a<a href=\"https://docs.traefik.io/providers/kubernetes-ingress/\"> Kubernetes Ingress controller</a>. Traefik includes a<a href=\"https://docs.traefik.io/https/acme/\"> built-in ACME client</a>, so no additional software is needed to begin working with Let’s Encrypt.</p><p>When used as an Ingress controller, Traefik ferries requests from the external network to services running within the Kubernetes cluster, using routes defined by Kubernetes<a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\"> Ingress resources</a>. Traefik’s ACME client can automatically request and provision certificates for any domain assigned to the cluster, making it possible to add TLS-encrypted routes.</p><p>Traefik aims to reduce setup times by automatically discovering the right configuration for a given infrastructure, so putting the pieces in place is straightforward. Instructions on how to install Traefik into a Kubernetes cluster and enable Let’s Encrypt are available in the<a href=\"https://docs.traefik.io/user-guides/crd-acme/\"> official documentation</a>.</p><h2 id=\"a-secure-starting-point\"><strong>A secure starting point</strong></h2><p>Traefik and Let’s Encrypt together offer to set up basic TLS routing for Kubernetes. As mentioned earlier, however, encryption for network security is a non-trivial matter, particularly in more complex environments. Just for starters, you’ll need genuine domain names that point to servers that are accessible by the public internet for Let’s Encrypt’s domain-validation challenge process to work.</p><p>Further challenges arise when you want to run more than one instance of Traefik, to ensure high availability. It’s currently not possible to run more than one instance of the community-maintained version of Traefik on a cluster with Let’s Encrypt enabled, because there’s no guarantee that any request will be handled by the correct instance of Traefik. One way to address this is to use the commercial<a href=\"https://containous.ghost.io/traefikee/\"> Traefik Enterprise Edition (TraefikEE)</a> product, which offers distributed Let’s Encrypt as a supported feature.</p><!--kg-card-begin: markdown--><p>Organizations with more specific needs – such as interfacing with enterprise vault providers – and who aren’t afraid to take a hands-on, DIY approach may need to install additional, dedicated certificate management software. One such option that’s proven popular is <a href=\"https://cert-manager.io/docs/\" target=\"_blank\" rel=\"nofollow\">cert-manager</a>, which can work in tandem with both Traefik and TraefikEE.</p>\n<!--kg-card-end: markdown--><p>The key point, however, is that TLS encryption should be everywhere, and today it can be. The ACME protocol, Let’s Encrypt, and self-configuring software like Traefik can automate away much of the hard work that was once involved with certificate management and secure request routing. In turn, that frees you to concentrate on what really matters, which is how to deliver the most value with your services, rather than how to secure them.</p><p>You can try out the features of TraefikEE, including distributed Let’s Encrypt, by starting a <a href=\"https://info.containo.us/get-traefik-enterprise-edition-free-for-30-days\">30-day free trial</a>.</p>","url":"https://containous.ghost.io/blog/simplified-security-for-kubernetes-with-traefik-and-lets-encrypt/","canonical_url":null,"uuid":"45235829-73db-4937-a45e-83147ba3995b","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5f161889d90a6e003946cbaa","reading_time":4}},{"node":{"id":"Ghost__Post__5ef263fe60f25700399d089d","title":"Beyond Kubernetes: Bringing Microservices Together with Service Mesh","slug":"beyond-kubernetes-bringing-microservices-together-with-service-mesh","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/06/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/9c9a2faffb966d5a2991625087a01021/47498/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh.jpg","srcSet":"/static/9c9a2faffb966d5a2991625087a01021/9dc27/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh.jpg 300w,\n/static/9c9a2faffb966d5a2991625087a01021/4fe8c/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh.jpg 600w,\n/static/9c9a2faffb966d5a2991625087a01021/47498/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh.jpg 1200w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"When adopting microservices, Kubernetes alone may not be enough to handle more complex networking challenges that arise. This is the job of a service mesh.","custom_excerpt":"When adopting microservices, Kubernetes alone may not be enough to handle more complex networking challenges that arise. This is the job of a service mesh.","visibility":"public","created_at_pretty":"23 June, 2020","published_at_pretty":"June 24, 2020","updated_at_pretty":"16 July, 2020","created_at":"2020-06-23T20:20:14.000+00:00","published_at":"2020-06-24T06:43:11.000+00:00","updated_at":"2020-07-16T13:26:09.000+00:00","meta_title":"Beyond Kubernetes: Bringing Microservices Together with Service Mesh","meta_description":"When adopting microservices, Kubernetes alone may not be enough to handle more complex networking challenges that arise. This is the job of a service mesh.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/06/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh---Twitter.jpg","twitter_title":null,"authors":[{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"When adopting the microservices application model, Kubernetes is a natural\nstarting point. Extensible, open source, and with a thriving ecosystem,\nKubernetes has emerged as the go-to orchestrator for containerized\ninfrastructure. When used as the foundation for microservices, however, you may\nfind that Kubernetes alone isn’t enough to handle the more complex networking\nchallenges that arise. This is the job of a service mesh.\n\nOne of the most important aspects to consider about the microservices model is\nits heavy dependence on networking. Unfortunately, a network is seldom as\nreliable or resilient as its hypothetical diagram suggests. Services fail,\nnetwork routes change or disappear, and unexpected traffic can disrupt normal\nusage patterns. This is even more true for containerized microservices, which by\ntheir nature tend to be stateless, ephemeral, and disposable. Maintaining the\nperformance and stability of such an environment is anything but simple.\nKubernetes addresses the first part of this challenge by automating the\nlifecycles of containers and their associated applications. When coupled with an \nIngress controller\n[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/],\nsuch as Contour or Traefik [/traefik/], Kubernetes also manages communications\nfrom the external network to workloads running in the cluster and vice versa\n(sometimes called north-south traffic).\n\nService Mesh BasicsOf equal importance in a microservices environment, however,\nare communications between services within the cluster (known as east-west \ntraffic). While basic networking within the cluster is handled by Kubernetes\nitself, a service mesh is a dedicated infrastructure layer that handles many of\nthe routine networking tasks that are necessary for a loose collection of\ncontainerized services to work together as a cohesive application.\n\nThe Role of Service Mesh\nAn important property of a service mesh is that it decouples east-west\nnetworking functions from application logic. As the environment scales and new\nservices are added to the mix, they should be able to expect the same level of\nmanagement as their peers, without code changes or refactoring. Examples of\nfunctions of a service mesh include:\n\nTraffic control. Communications between the cluster and the external network\nrequire routing and management, and so do communications between services on the\ncluster. Tasks such as advanced load balancing\n[https://en.wikipedia.org/wiki/Load_balancing_(computing)] and rate limiting\n[https://cloud.google.com/solutions/rate-limiting-strategies-techniques] of\nservice-to-service traffic are primary functions of the service mesh, which\nhelps to prevent and contain disruptions and performance degradations that\nmisconfigured or misbehaving services can cause.\n\nSecurity. Even when ingress controls are in place to protect the cluster from\nexternal networking threats, attackers may still attempt to exploit trusted\nrelationships between services on the cluster. A service mesh can help here by\nproviding seamless authentication, access control, and management of encrypted\nlinks between services, among other security features.\n\nObservability. Another important factor is the ability of a service mesh to\nprovide consistent logging, metrics, and visibility into the inner workings of a\ncluster. The insights gained from these functions are invaluable for maintaining\nthe health and proper operation of microservices-based applications.\n\nOptions and Standards\nOver the years, various forms of application middleware have implemented\neast-west networking functions in various ways. It is only since the rise of\ncontainerized infrastructure, and Kubernetes in particular, that the concept of\na service mesh as a dedicated layer has truly crystallized. Even so, opinions on\nhow a mesh should be implemented still differ.\n\nStandardization offers some hope for clarity. Recently, a consortium of\ncloud-native software vendors has begun collaborating on the Service Mesh\nInterface (SMI) [https://smi-spec.io/], an evolving effort that is shepherded by\nthe Cloud Native Computing Foundation (CNCF) [https://cncf.io/]. The SMI\nspecification defines APIs for many of the networking functions described\nearlier. Still, these APIs can be implemented in multiple ways.\n\nThe sidecar pattern\n[https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/#example-1-sidecar-containers] \nis one way to design a service mesh for Kubernetes. In this model, a so-called\nsidecar container is deployed alongside each instance of a service within a pod\nto handle east-west traffic for that instance. This is a popular pattern for\nimplementing a service mesh, and is the method adopted by the likes of Istio and\nLinkerd (the latter a CNCF incubating project).\n\nAn alternative method is to deploy a service mesh proxy endpoint that runs as\nits own pod on each node of a cluster, using the Kubernetes concept of a \nDaemonSet [https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]\n. This method has the advantage of being less invasive, in that the mesh proxy\ndoes not need to modify any Kubernetes objects and no network traffic is\nmodified without a service owner’s consent. This is the model adopted by Maesh\n[/maesh/].\n\nHow to Move Forward\nSo, while standardization helps make it easier to know what to expect from a\nservice mesh, important decisions remain before adopting a specific solution.\nEven choosing how to begin deploying a mesh can be challenging, owing to the\npotential for disruption of existing communication patterns within a cluster.\n\nThese decisions are made significantly easier when dealing with so-called\ngreenfield projects, where the clustered, microservices-based application is\nbuilt from the ground up, without dependencies on legacy infrastructure. This is\na happy situation to have, but it’s not always realistic, especially in\nenvironments where adoption of containerization is already mature.\n\nIf it will be necessary to run some services on a service mesh alongside other\nservices for which it is preferable to have them manage their own east-west\ntraffic, it may be worth looking for a loosely coupled solution, such as Maesh.\nThis has the advantage of allowing the teams that own individual services to opt\ninto the mesh when they are ready, rather than forcing a mass migration with\ninterdependencies that may be difficult to test.\n\nHowever you choose to proceed, the key takeaway should be that service mesh for\nKubernetes, while still an emerging technology, can often provide the “missing\npiece” that Kubernetes alone does not. By abstracting key east-west networking\nfeatures away from application logic, mesh solutions enable distributed,\nmicroservices-based applications to operate in a way that is managed,\nobservable, and secure, both when communicating with the outside world and\nwithin the cluster itself.\n\nTo learn more, check out this video\n[https://info.containo.us/video-understanding-service-mesh] about the advantages\nand disadvantages of a service mesh, and the appropriate situations for using\none.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/06/Beyond-Kubernetes-Bringing-Microservices-Together-with-Service-Mesh-1.jpg\" class=\"kg-image\" alt=\"Bringing microservices together with service mesh\"></figure><p>When adopting the microservices application model, Kubernetes is a natural starting point. Extensible, open source, and with a thriving ecosystem, Kubernetes has emerged as the go-to orchestrator for containerized infrastructure. When used as the foundation for microservices, however, you may find that Kubernetes alone isn’t enough to handle the more complex networking challenges that arise. This is the job of a <strong>service mesh</strong>.</p><p>One of the most important aspects to consider about the microservices model is its heavy dependence on networking. Unfortunately, a network is seldom as reliable or resilient as its hypothetical diagram suggests. Services fail, network routes change or disappear, and unexpected traffic can disrupt normal usage patterns. This is even more true for containerized microservices, which by their nature tend to be stateless, ephemeral, and disposable. Maintaining the performance and stability of such an environment is anything but simple. Kubernetes addresses the first part of this challenge by automating the lifecycles of containers and their associated applications. When coupled with an <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress controller</a>, such as Contour or <a href=\"https://containous.ghost.io/traefik/\">Traefik</a>, Kubernetes also manages communications from the external network to workloads running in the cluster and vice versa (sometimes called <em>north-south</em> traffic).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/06/Service-Mesh-Diagram@2x.jpg\" class=\"kg-image\" alt=\"Service Mesh Basics\"><figcaption>Service Mesh Basics</figcaption></figure><p>Of equal importance in a microservices environment, however, are communications between services within the cluster (known as <em>east-west</em> traffic). While basic networking within the cluster is handled by Kubernetes itself, a service mesh is a dedicated infrastructure layer that handles many of the routine networking tasks that are necessary for a loose collection of containerized services to work together as a cohesive application.</p><h2 id=\"the-role-of-service-mesh\">The Role of Service Mesh</h2><p>An important property of a service mesh is that it decouples east-west networking functions from application logic. As the environment scales and new services are added to the mix, they should be able to expect the same level of management as their peers, without code changes or refactoring. Examples of functions of a service mesh include:</p><!--kg-card-begin: markdown--><p><strong>Traffic control.</strong> Communications between the cluster and the external network require routing and management, and so do communications between services on the cluster. Tasks such as advanced <a href=\"https://en.wikipedia.org/wiki/Load_balancing_(computing)\" target=\"_blank\" rel=\"nofollow\">load balancing</a> and <a href=\"https://cloud.google.com/solutions/rate-limiting-strategies-techniques\" target=\"_blank\" rel=\"nofollow\">rate limiting</a> of service-to-service traffic are primary functions of the service mesh, which helps to prevent and contain disruptions and performance degradations that misconfigured or misbehaving services can cause.</p>\n<!--kg-card-end: markdown--><p><strong>Security. </strong>Even when ingress controls are in place to protect the cluster from external networking threats, attackers may still attempt to exploit trusted relationships between services on the cluster. A service mesh can help here by providing seamless authentication, access control, and management of encrypted links between services, among other security features.</p><p><strong>Observability. </strong>Another important factor is the ability of a service mesh to provide consistent logging, metrics, and visibility into the inner workings of a cluster. The insights gained from these functions are invaluable for maintaining the health and proper operation of microservices-based applications.</p><h2 id=\"options-and-standards\">Options and Standards</h2><p>Over the years, various forms of application middleware have implemented east-west networking functions in various ways. It is only since the rise of containerized infrastructure, and Kubernetes in particular, that the concept of a service mesh as a dedicated layer has truly crystallized. Even so, opinions on how a mesh should be implemented still differ.</p><p>Standardization offers some hope for clarity. Recently, a consortium of cloud-native software vendors has begun collaborating on the<a href=\"https://smi-spec.io/\"> Service Mesh Interface (SMI)</a>, an evolving effort that is shepherded by the Cloud Native<a href=\"https://cncf.io/\"> Computing Foundation (CNCF)</a>. The SMI specification defines APIs for many of the networking functions described earlier. Still, these APIs can be implemented in multiple ways.</p><!--kg-card-begin: markdown--><p>The <a href=\"https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/#example-1-sidecar-containers\" target=\"_blank\" rel=\"nofollow\">sidecar pattern</a> is one way to design a service mesh for Kubernetes. In this model, a so-called sidecar container is deployed alongside each instance of a service within a pod to handle east-west traffic for that instance. This is a popular pattern for implementing a service mesh, and is the method adopted by the likes of Istio and Linkerd (the latter a CNCF incubating project).</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>An alternative method is to deploy a service mesh proxy endpoint that runs as its own pod on each node of a cluster, using the Kubernetes concept of a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\" target=\"_blank\" rel=\"nofollow\">DaemonSet</a>. This method has the advantage of being less invasive, in that the mesh proxy does not need to modify any Kubernetes objects and no network traffic is modified without a service owner’s consent. This is the model adopted by <a href=\"https://containous.ghost.io/maesh/\">Maesh</a>.</p>\n<!--kg-card-end: markdown--><h2 id=\"how-to-move-forward\">How to Move Forward</h2><p>So, while standardization helps make it easier to know what to expect from a service mesh, important decisions remain before adopting a specific solution. Even choosing how to begin deploying a mesh can be challenging, owing to the potential for disruption of existing communication patterns within a cluster.</p><p>These decisions are made significantly easier when dealing with so-called greenfield projects, where the clustered, microservices-based application is built from the ground up, without dependencies on legacy infrastructure. This is a happy situation to have, but it’s not always realistic, especially in environments where adoption of containerization is already mature.</p><p>If it will be necessary to run some services on a service mesh alongside other services for which it is preferable to have them manage their own east-west traffic, it may be worth looking for a loosely coupled solution, such as Maesh. This has the advantage of allowing the teams that own individual services to opt into the mesh when they are ready, rather than forcing a mass migration with interdependencies that may be difficult to test.</p><p>However you choose to proceed, the key takeaway should be that service mesh for Kubernetes, while still an emerging technology, can often provide the “missing piece” that Kubernetes alone does not. By abstracting key east-west networking features away from application logic, mesh solutions enable distributed, microservices-based applications to operate in a way that is managed, observable, and secure, both when communicating with the outside world and within the cluster itself.</p><p>To learn more, <a href=\"https://info.containo.us/video-understanding-service-mesh\">check out this video</a> about the advantages and disadvantages of a <strong>service mesh</strong>, and the appropriate situations for using one.</p>","url":"https://containous.ghost.io/blog/beyond-kubernetes-bringing-microservices-together-with-service-mesh/","canonical_url":null,"uuid":"4b55f40d-0afe-4880-8a03-6815bd0e7bb9","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ef263fe60f25700399d089d","reading_time":4}},{"node":{"id":"Ghost__Post__5ee81ca7292c470045af26fe","title":"Securing your Kubernetes environment against external traffic threats","slug":"securing-your-kubernetes-environment-against-external-traffic-threats","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/06/Securing-your-Kubernetes-environment-against-external-traffic-threats-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/c597de81abf63dccc70c384729c03cf7/47498/Securing-your-Kubernetes-environment-against-external-traffic-threats-1.jpg","srcSet":"/static/c597de81abf63dccc70c384729c03cf7/9dc27/Securing-your-Kubernetes-environment-against-external-traffic-threats-1.jpg 300w,\n/static/c597de81abf63dccc70c384729c03cf7/4fe8c/Securing-your-Kubernetes-environment-against-external-traffic-threats-1.jpg 600w,\n/static/c597de81abf63dccc70c384729c03cf7/47498/Securing-your-Kubernetes-environment-against-external-traffic-threats-1.jpg 1200w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"Kubernetes is often used to manage external-facing applications, so the need for protecting applications from harmful external traffic is nearly universal.","custom_excerpt":"Kubernetes is often used to manage external-facing applications, so the need for protecting applications from harmful external traffic is nearly universal.","visibility":"public","created_at_pretty":"16 June, 2020","published_at_pretty":"June 16, 2020","updated_at_pretty":"10 September, 2020","created_at":"2020-06-16T01:13:11.000+00:00","published_at":"2020-06-16T14:27:27.000+00:00","updated_at":"2020-09-10T05:14:27.000+00:00","meta_title":"Securing Kubernetes environment against external traffic threats","meta_description":"Kubernetes is often used to manage external-facing applications, so the need for protecting applications from harmful external traffic is nearly universal.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/06/Securing-your-Kubernetes-environment-against-external-traffic-threats-Twitter.jpg","twitter_title":null,"authors":[{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Neil McAllister","slug":"neil","bio":null,"profile_image":"https://containous.ghost.io/content/images/2020/05/Neil_McAllister_GPS_sm.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Kubernetes delivers many quality-of-life improvements for software engineering\nteams out of the box by automating the deployment and lifecycle management of\ncontainerized applications. However, security is one area where developers must\nstill be proactive, by identifying threat models and applying appropriate\nKubernetes security best practices to address them. \n\nGiven that Kubernetes is often used to manage external-facing applications, the\nneed for protecting applications from harmful external traffic is nearly\nuniversal. In this article we’ll review some of the general concerns that\ndevelopers should keep in mind in this context, and then discuss relevant\nKubernetes security approaches.\n\nTop external traffic concerns\nWhen it comes to security issues from external traffic, there are myriad\nconcerns that range from unintentional but detrimental behavior to organized,\nmalevolent attacks.\n\nUnfair usage\nAny application of even moderate complexity will exhibit significant variance in\nresource usage and execution time of requests. For example, certain operations\nmight trigger backend batch jobs or invoke complicated queries to datastores.\nAccordingly, the overall load imposed by a specific external client will be a\nfunction of the request types it sends and their distribution over time (for\nexample, bursty versus parallel requests). Because of this, it’s possible for a\nclient to unintentionally use an unfair portion of provisioned resources and\nnegatively impact other users.\n\nMisconfigured clients\nParticularly for API services, it’s common for incoming external traffic to have\nbeen generated by automated systems or client software whose behavior is driven\nby user-specific configuration settings. Settings that are misconfigured –\nwhether maliciously or unintentionally – can lead to anomalous behavior, which\ncan easily lead to undesirable load on the Kubernetes-managed application.\n\nMalicious clients\nAn unfortunate reality for any internet-facing application is that it will\ninevitably face nefarious attempts to identify and/or exploit potential security\nvulnerabilities in the software and gain some form of unauthorized access. Such\nattacks include zero-day exploits and brute-force attempts to penetrate the\nsystem, among others. If successful, these attacks can have significant negative\nconsequences beyond application downtime, including data breaches.\n\nDenial-of-service (DDoS) and distributed denial-of-service (DDoS)\nA malicious attacker may also intentionally try to limit or disrupt the resource\navailability for legitimate traffic, in what is known as a denial-of-service\n(DoS) attack. Such attacks often combine several of the previously mentioned\nelements. In their more sophisticated form, known as distributed\ndenial-of-service (DDoS), they can also be orchestrated such that a distributed\nand possible dynamic set of clients works together to send malformed or\nexcessive traffic simultaneously.\n\nMitigation strategies\nHaving reviewed the top concerns that harmful external traffic creates for\napplications, let’s look at some approaches to mitigating them. Specifically, by\nemploying an Ingress controller for Kubernetes\n[https://containo.us/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/]\n, multiple options become available to consider as part of a comprehensive\nsecurity strategy.\n\nIP allow/deny lists\nOne approach to integrating security best practices is to employ network-level\nmechanisms. Specifically, the IP addresses of external traffic can be used to\nmake binary decisions on whether the traffic should be allowed at all. If\napplications have a stable set of known clients, a proactive allow list approach\nin which unknown IP addresses are ignored may be applicable. On the other hand,\nif valid traffic could come from anywhere, a reactive deny list can be\nmaintained based upon the identification of malicious traffic sources.\n\nConnection limits\nAnother network-level option is to limit the number of connections that an\nexternal client can create, which can help limit the impact of misconfigured\nclients as well as malicious users. One potential challenge, however, lies in\ndetermining an appropriate value to set for the connection limit. Many\nimplementations require a global setting that impacts all sources of traffic.\nParticularly when imposing connection limits by source IP address, scenarios in\nwhich multiple clients may share an IP address – such as NAT, for example –\nshould be considered.\n\nRate limiting\nMoving up the networking stack, an Ingress controller can be used to enable rate\nlimiting at the request level. These limits gate the number of requests in a\ngiven time window (for example, requests per second) and can be enforced in a\nvariety of ways. Common options include setting rate limits based upon client IP\naddress or a value in the HTTP header. Rate limits can also be configured so a\nmaximum request rate is enforced at service hosts.\n\nLimiting simultaneous requests\nAs mentioned earlier, request-processing times may vary based on a variety of\nfactors. While rate limits can enforce the number of external requests submitted\nby a client in a given time frame, the number of concurrent requests being\nprocessed may vary significantly depending upon the relative processing time of\neach type of request. To address this dimension, external traffic can be managed\nby setting limits so that services don’t become overwhelmed by a large number of\nsimultaneous, resource-intensive requests.\n\nAudit logging\nA question that often arises is how operations engineers can determine when the\naforementioned mitigation mechanisms should be invoked, as well as their overall\neffectiveness. For example, it’s preferable to detect DDoS attacks in their\nearliest stages to minimize disruption to end users. How to spot them when they\nstart? Access logs can be invaluable for this purpose. They are commonly\nincorporated as part of a broader security information and event management\n(SIEM) system. When employed with Kubernetes Ingress controllers like Traefik,\nthese logs also make it possible to automate remediation strategies based upon\ndetected external traffic patterns.\n\nStay vigilant\nWhen considering how best to protect workloads from harmful external traffic,\nthe most effective approach will likely require a combination of several of the\nmechanisms we’ve discussed. But which ones? The need for any one measure may not\nbe immediately apparent, so its utility should be continuously reviewed with the\nhelp of data from audit logs. It’s important that applications teams consider\nthe ability of their Kubernetes platform to provide a broad set of mechanisms\nthat can be called upon as security requirements change and evolve. \n\nSolutions like TraefikEE [https://containo.us/traefikee/] align well with this\nmindset by providing a rich and robust set of security features for managing\nexternal traffic embedded in the Kubernetes Ingress controller\n[https://containo.us/solutions/kubernetes-ingress/]. Learn more about TraefikEE\nin this 5-minute video [https://info.containo.us/request-demo-traefikee].","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/06/Securing-your-Kubernetes-environment-against-external-traffic-threats.jpg\" class=\"kg-image\" alt=\"Securing your Kubernetes environment\"></figure><p>Kubernetes delivers many quality-of-life improvements for software engineering teams out of the box by automating the deployment and lifecycle management of containerized applications. However, security is one area where developers must still be proactive, by identifying threat models and applying appropriate Kubernetes security best practices to address them. </p><p>Given that Kubernetes is often used to manage external-facing applications, the need for protecting applications from harmful external traffic is nearly universal. In this article we’ll review some of the general concerns that developers should keep in mind in this context, and then discuss relevant Kubernetes security approaches.</p><h2 id=\"top-external-traffic-concerns\">Top external traffic concerns</h2><p>When it comes to security issues from external traffic, there are myriad concerns that range from unintentional but detrimental behavior to organized, malevolent attacks.</p><h3 id=\"unfair-usage\">Unfair usage</h3><p>Any application of even moderate complexity will exhibit significant variance in resource usage and execution time of requests. For example, certain operations might trigger backend batch jobs or invoke complicated queries to datastores. Accordingly, the overall load imposed by a specific external client will be a function of the request types it sends and their distribution over time (for example, bursty versus parallel requests). Because of this, it’s possible for a client to unintentionally use an unfair portion of provisioned resources and negatively impact other users.</p><h3 id=\"misconfigured-clients\">Misconfigured clients</h3><p>Particularly for API services, it’s common for incoming external traffic to have been generated by automated systems or client software whose behavior is driven by user-specific configuration settings. Settings that are misconfigured – whether maliciously or unintentionally – can lead to anomalous behavior, which can easily lead to undesirable load on the Kubernetes-managed application.</p><h3 id=\"malicious-clients\">Malicious clients</h3><p>An unfortunate reality for any internet-facing application is that it will inevitably face nefarious attempts to identify and/or exploit potential security vulnerabilities in the software and gain some form of unauthorized access. Such attacks include zero-day exploits and brute-force attempts to penetrate the system, among others. If successful, these attacks can have significant negative consequences beyond application downtime, including data breaches.</p><h3 id=\"denial-of-service-ddos-and-distributed-denial-of-service-ddos-\">Denial-of-service (DDoS) and distributed denial-of-service (DDoS)</h3><p>A malicious attacker may also intentionally try to limit or disrupt the resource availability for legitimate traffic, in what is known as a denial-of-service (DoS) attack. Such attacks often combine several of the previously mentioned elements. In their more sophisticated form, known as distributed denial-of-service (DDoS), they can also be orchestrated such that a distributed and possible dynamic set of clients works together to send malformed or excessive traffic simultaneously.</p><h2 id=\"mitigation-strategies\">Mitigation strategies</h2><p>Having reviewed the top concerns that harmful external traffic creates for applications, let’s look at some approaches to mitigating them. Specifically, by employing an <a href=\"https://containo.us/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/\">Ingress controller for Kubernetes</a>, multiple options become available to consider as part of a comprehensive security strategy.</p><h3 id=\"ip-allow-deny-lists\">IP allow/deny lists</h3><p>One approach to integrating security best practices is to employ network-level mechanisms. Specifically, the IP addresses of external traffic can be used to make binary decisions on whether the traffic should be allowed at all. If applications have a stable set of known clients, a proactive allow list approach in which unknown IP addresses are ignored may be applicable. On the other hand, if valid traffic could come from anywhere, a reactive deny list can be maintained based upon the identification of malicious traffic sources.</p><h3 id=\"connection-limits\">Connection limits</h3><p>Another network-level option is to limit the number of connections that an external client can create, which can help limit the impact of misconfigured clients as well as malicious users. One potential challenge, however, lies in determining an appropriate value to set for the connection limit. Many implementations require a global setting that impacts all sources of traffic. Particularly when imposing connection limits by source IP address, scenarios in which multiple clients may share an IP address – such as NAT, for example – should be considered.</p><h3 id=\"rate-limiting\">Rate limiting</h3><p>Moving up the networking stack, an Ingress controller can be used to enable rate limiting at the request level. These limits gate the number of requests in a given time window (for example, requests per second) and can be enforced in a variety of ways. Common options include setting rate limits based upon client IP address or a value in the HTTP header. Rate limits can also be configured so a maximum request rate is enforced at service hosts.</p><h3 id=\"limiting-simultaneous-requests\">Limiting simultaneous requests</h3><p>As mentioned earlier, request-processing times may vary based on a variety of factors. While rate limits can enforce the number of external requests submitted by a client in a given time frame, the number of concurrent requests being processed may vary significantly depending upon the relative processing time of each type of request. To address this dimension, external traffic can be managed by setting limits so that services don’t become overwhelmed by a large number of simultaneous, resource-intensive requests.</p><h3 id=\"audit-logging\">Audit logging</h3><p>A question that often arises is how operations engineers can determine when the aforementioned mitigation mechanisms should be invoked, as well as their overall effectiveness. For example, it’s preferable to detect DDoS attacks in their earliest stages to minimize disruption to end users. How to spot them when they start? Access logs can be invaluable for this purpose. They are commonly incorporated as part of a broader security information and event management (SIEM) system. When employed with Kubernetes Ingress controllers like Traefik, these logs also make it possible to automate remediation strategies based upon detected external traffic patterns.</p><h2 id=\"stay-vigilant\">Stay vigilant</h2><p>When considering how best to protect workloads from harmful external traffic, the most effective approach will likely require a combination of several of the mechanisms we’ve discussed. But which ones? The need for any one measure may not be immediately apparent, so its utility should be continuously reviewed with the help of data from audit logs. It’s important that applications teams consider the ability of their Kubernetes platform to provide a broad set of mechanisms that can be called upon as security requirements change and evolve. </p><!--kg-card-begin: markdown--><p>Solutions like <a href=\"https://containo.us/traefikee/\">TraefikEE</a> align well with this mindset by providing a rich and robust set of security features for managing external traffic embedded in the <a href=\"https://containo.us/solutions/kubernetes-ingress/\">Kubernetes Ingress controller</a>. Learn more about TraefikEE in <a href=\"https://info.containo.us/request-demo-traefikee\" target=\"_blank\" rel=\"nofollow\">this 5-minute video</a>.</p>\n<!--kg-card-end: markdown-->","url":"https://containous.ghost.io/blog/securing-your-kubernetes-environment-against-external-traffic-threats/","canonical_url":null,"uuid":"2da8c425-5cf1-4594-83bf-f9732530e7e0","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ee81ca7292c470045af26fe","reading_time":4}},{"node":{"id":"Ghost__Post__5ec2a072c49e39004576b7ad","title":"Kubernetes Ingress & Service API Demystified","slug":"kubernetes-ingress-service-api-demystified","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/10/Kubernetes-Ingress---Service-API-Demystified-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/d9aac10cdc9e917545bdb3e3680ce4f2/47498/Kubernetes-Ingress---Service-API-Demystified-1.jpg","srcSet":"/static/d9aac10cdc9e917545bdb3e3680ce4f2/9dc27/Kubernetes-Ingress---Service-API-Demystified-1.jpg 300w,\n/static/d9aac10cdc9e917545bdb3e3680ce4f2/4fe8c/Kubernetes-Ingress---Service-API-Demystified-1.jpg 600w,\n/static/d9aac10cdc9e917545bdb3e3680ce4f2/47498/Kubernetes-Ingress---Service-API-Demystified-1.jpg 1200w,\n/static/d9aac10cdc9e917545bdb3e3680ce4f2/52258/Kubernetes-Ingress---Service-API-Demystified-1.jpg 1800w,\n/static/d9aac10cdc9e917545bdb3e3680ce4f2/a41d1/Kubernetes-Ingress---Service-API-Demystified-1.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"The Ingress Object itself already has a long history with K8s. It is still considered beta, which is kinda surprising for something that has been so long present in K8s. But why is that? And when will that change?","custom_excerpt":"The Ingress Object itself already has a long history with K8s. It is still considered beta, which is kinda surprising for something that has been so long present in K8s. But why is that? And when will that change?","visibility":"public","created_at_pretty":"18 May, 2020","published_at_pretty":"June 2, 2020","updated_at_pretty":"14 October, 2020","created_at":"2020-05-18T14:49:22.000+00:00","published_at":"2020-06-02T14:24:25.000+00:00","updated_at":"2020-10-14T05:00:57.000+00:00","meta_title":"Kubernetes Ingress & Service API Demystified","meta_description":"This blog covers some of the long-standing issues with the current state of Ingress in Kubernetes, as well as the new Service API aimed at solving them.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/10/Kubernetes-Ingress---Service-API-Demystified-2.jpg","twitter_title":null,"authors":[{"name":"Manuel Zapf","slug":"manuel","bio":null,"profile_image":"https://containous.ghost.io/content/images/2019/11/b_manuel-zapf-foto.1024x1024.png","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Manuel Zapf","slug":"manuel","bio":null,"profile_image":"https://containous.ghost.io/content/images/2019/11/b_manuel-zapf-foto.1024x1024.png","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"The Ingress Object itself already has a long history with K8s. It is still\nconsidered beta, which is kinda surprising for something that has been so long\npresent in K8s. But why is that? And when will that change?\n\nWith the release of Kubernetes 1.18, some improvements\n[https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/] \nhave been made to Ingress, which have been overdue for a long time. However, the\nchanges introduced are minor, and some of the issues we’ll be covering in this\nblog post have gone untackled. In addition to covering the issues mentioned\nabove, we’ll be exploring the new Service API aimed at solving these issues.\n\nIssues\nIn this blog post we’ll cover some of the long-standing issues with the current\nstate of Ingress in Kubernetes, including these topics:\n\n * Inflexible HTTP routing definitions\n * Schema differences across vendors\n * Extensibility of Ingress\n\nInflexible HTTP routing definitions\nA simple Ingress object example is the following:\n\n---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - Host: myhost.com\n  path: /testpath\n        pathType: Prefix\n        backend:\nserviceName: test\n \tservicePort: 80\n\n\nThe above Ingress object will route HTTP requests with the URI  GET \nhttp://myhost.com/testpath and forward the request internally to the service\ncalled test on port 80. So far, so good.\n\nThe primary focus of an ingress resource is on solving simple HTTP routing\ncases, similar to the concept of Virtual Hosts with a Path Based routing\nextension. This leads us to the first issue: How would you configure cases like\na redirect?\n\nSchema differences across vendors\nFurther configuration is usually done through annotations on the Ingress\nresource. Annotations are like key-value pairs stored in the metadata of an\nobject.\n\n---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n      paths:\n      - Host: myhost.com\n  path: /testpath\n        pathType: Prefix\n        backend:\nserviceName: test\n \tservicePort: 80\n\n\nThe above example shows that the Ingress Controller (nginx) would rewrite all\nrequests to / (slash) before forwarding to the backend.\n\nConsider that the same configuration in Traefik would look like this:\n\n--- \napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata: \n  name: rewrite-slash\nspec: \n  replacePath: \n    path: /\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: whoami-ingress\n  annotations:\n    kubernetes.io/ingress.class: traefik              \n    traefik.ingress.kubernetes.io/router.middlewares: default-rewrite-slash@kubernetescrd\nspec:\n  rules:\n  - host: whoami.localhost\n    http:\n      paths:\n      - path: /foobar\n        backend:\n          serviceName: whoami\n          servicePort: web\n\n\n\nAs you can see, it’s totally different! Which brings us to another issue:\nextensibility. \n\nExtensibility of Ingress\nExtending Ingress Objects is a requirement and the de-facto standard for that is\nusing annotations. However, annotations often differ between the many different\nimplementations of  Ingress Controllers and it's therefore hard to manage for an\nend-user.\n\nThis unmanageability also translates back to the providers who must maintain\ntheir ingress controller implementation, who are often constrained by the\nsimplicity of the key / value pair approach.\n\nService API aka Ingress V2\nAnnounced at Kubecon NA in 2019 by Google there has been substantial effort in\n creating an “Ingress V2” which is now known as the Service API\n[https://github.com/kubernetes-sigs/service-apis].\n\nThis specification aims to solve a few problems:\n\n * Provide clean separation and role-based control\n * Uplevel the Ingress specification\n * Specify standard methods of extending the Ingress specification\n\nTo do so, the specification currently consists of 4 different CRD Ressources:\n\nGatewayclass\nThe GatewayClass is meant to be a Cluster Scoped resource, which is meant to\nrepresent a “category” of gateways. It’s similar to the former `ingress.class`\nannotation or the now included IngressClass resource.\n\nThe expected use-case is to have more than one GatewayClass per Ingress\nController provider. These classes may have a variety of default settings, which\nare inherited by the cluster-level gateway. Also, this can be used to pass\nadditional configuration down to that gateway. Since it is a cluster scoped\nresource, it's expected to be managed by the Infrastructure provider.\n\n---\nkind: GatewayClass\nmetadata:\n  name: cluster-gateway\nspec:\n  controller: \"acme.io/gateway-controller\"\n  parametersRef:\n    apiVersion: core/v1\n    kind: ConfigMap\n    namespace: acme-system\n    name: internet-gateway\n\n\nGateway\nThe gateway has a life-cycle which is tied with the infrastructure. For\ninstance, one Gateway could be running an instance of Traefik or one AWS ELB. As\nalready mentioned, it’s linked to a Gateway class for inferring configuration.\n\nThe gateway sets listener bindings (Address, Ports, TLS…) and the routes served\nby the gateway.\n\n---\nkind: Gateway\nname: my-app-gw\nnamespace: my-app\nspec:\n  class: from-internet\n  listeners:\n  - address:\n      ip: 1.2.3.4\n    protocols: [\"http\"] # implies port 80.\n    routes:\n      ...\n  - address:\n      ip: 1.2.3.4\n    protocols: [\"https\"] # implies port 443\n    certificates:\n    - name: my-secret\n    - apiGroup: certmanager.k8s.io\n      kind: Certificate               \n      name: lets-encrypt-cert\n    routes:\n      - route:\n        name: http-app-1\n        namespace: app-1\n        kind: HTTPRoute\n\n\nIt also has some sane default protocols like: http, https, TCP… which map to\npredefined ports.\n\nRoute\nLast but not least, a route is used to describe a way to handle traffic given\nprotocol level descriptions.\n\nA route can be of a different ressource (HTTPRoute, TLSRoute, TCPRoute…) and can\ntherefore have different protocol level descriptions taken into consideration\nfor routing. Each of these Protocols have different attributes which could be\nused for route matching. It can also be used to delegate to other Route\nResources in a multi-tenant scenario, for example, where  one team offers a\nglobal authentication service where you would want to forward from within your\ncurrent scope.\n\n---\nkind: HTTPRoute\nname: delegate-1\nnamespace: other-team\nrules:\n- match:\n    http:\n      host: bar.com\n      path:\n        prefix: /store\n  action:\n    backend:                           \n      name: delegate-1                  \n      namespace: other-team             \n      kind: HTTPRoute\n      apiGroup: networking.k8s.io\n\n\nIn the above HTTPRoute example it’s listening for traffic with a host of bar.com\nand looking for a Pathprefix of /store, which is close to some of the examples\nwe’ve explored on  traditional Ingress but allows for some additional specifics.\n\nExtensibility\nTaking what we just learned to the next level, the question that arrives now is:\n“How can we extend that, for example, to implement Rewrites?”\n\nFor that purpose, the specification currently offers three different levels of\nsupport:\n\n * Core\n * Extended\n * Custom\n\nCore functionality is guaranteed between all solutions respecting a specific set\nof API’s. Extended is a standardised API, but functionality is not guaranteed\nbetween all solutions.\n\nFor special use-cases, which might be vendor specific, there’s a custom layer\nwhere the implementation can be custom built to meet specific requirements.\nUsually, these will end up in CRD’s or custom annotations.\n\nSumUp\nService API is a new set of forward-looking API’s attempting to solve some\nissues that have become apparent over the evolving usage of Ingress. However, as\nit’s a bit more complex and might not solve all the simple use cases Ingress is\ncapable of solving, it’s not meant to replace Ingress but rather provide an\nalternative for complex use cases.\n\nEventually, Traefik itself will support the new Service API spec. In the\nmeantime, we offer support for both native Kubernetes Ingress and have extended\nsupport for Ingress through the use of CRDs. Learn more about how we do both\nand\nempower developers with flexible and easy to use Kubernetes Ingress\n[/solutions/kubernetes-ingress/].","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/10/Kubernetes-Ingress---Service-API-Demystified.jpg\" class=\"kg-image\" alt=\"Kubernetes Ingress &amp; Service API Demystified\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/10/Kubernetes-Ingress---Service-API-Demystified.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/10/Kubernetes-Ingress---Service-API-Demystified.jpg 1000w, https://containous.ghost.io/content/images/size/w1600/2020/10/Kubernetes-Ingress---Service-API-Demystified.jpg 1600w, https://containous.ghost.io/content/images/2020/10/Kubernetes-Ingress---Service-API-Demystified.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The Ingress Object itself already has a long history with K8s. It is still considered beta, which is kinda surprising for something that has been so long present in K8s. But why is that? And when will that change?</p><p>With the release of Kubernetes 1.18, some <a href=\"https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/\">improvements</a> have been made to Ingress, which have been overdue for a long time. However, the changes introduced are minor, and some of the issues we’ll be covering in this blog post have gone untackled. In addition to covering the issues mentioned above, we’ll be exploring the new Service API aimed at solving these issues.</p><h2 id=\"issues\">Issues</h2><p>In this blog post we’ll cover some of the long-standing issues with the current state of Ingress in Kubernetes, including these topics:</p><ul><li>Inflexible HTTP routing definitions</li><li>Schema differences across vendors</li><li>Extensibility of Ingress</li></ul><h2 id=\"inflexible-http-routing-definitions\">Inflexible HTTP routing definitions</h2><p>A simple Ingress object example is the following:</p><!--kg-card-begin: markdown--><pre><code>---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - Host: myhost.com\n  path: /testpath\n        pathType: Prefix\n        backend:\nserviceName: test\n \tservicePort: 80\n</code></pre>\n<!--kg-card-end: markdown--><p>The above Ingress object will route HTTP requests with the URI  GET <em>http://myhost.com/testpath</em> and forward the request internally to the service called test on port 80. So far, so good.</p><p>The primary focus of an ingress resource is on solving simple HTTP routing cases, similar to the concept of Virtual Hosts with a Path Based routing extension. This leads us to the first issue: How would you configure cases like a redirect?</p><h2 id=\"schema-differences-across-vendors\">Schema differences across vendors</h2><p>Further configuration is usually done through annotations on the Ingress resource. Annotations are like key-value pairs stored in the metadata of an object.</p><!--kg-card-begin: markdown--><pre><code>---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n      paths:\n      - Host: myhost.com\n  path: /testpath\n        pathType: Prefix\n        backend:\nserviceName: test\n \tservicePort: 80\n</code></pre>\n<!--kg-card-end: markdown--><p>The above example shows that the Ingress Controller (nginx) would rewrite all requests to / (slash) before forwarding to the backend.</p><p>Consider that the same configuration in Traefik would look like this:</p><!--kg-card-begin: markdown--><pre><code>--- \napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata: \n  name: rewrite-slash\nspec: \n  replacePath: \n    path: /\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: whoami-ingress\n  annotations:\n    kubernetes.io/ingress.class: traefik              \n    traefik.ingress.kubernetes.io/router.middlewares: default-rewrite-slash@kubernetescrd\nspec:\n  rules:\n  - host: whoami.localhost\n    http:\n      paths:\n      - path: /foobar\n        backend:\n          serviceName: whoami\n          servicePort: web\n\n</code></pre>\n<!--kg-card-end: markdown--><p>As you can see, it’s totally different! Which brings us to another issue: extensibility. </p><h2 id=\"extensibility-of-ingress\">Extensibility of Ingress</h2><p>Extending Ingress Objects is a requirement and the de-facto standard for that is using annotations. However, annotations often differ between the many different implementations of  Ingress Controllers and it's therefore hard to manage for an end-user.</p><p>This unmanageability also translates back to the providers who must maintain their ingress controller implementation, who are often constrained by the simplicity of the key / value pair approach.</p><h2 id=\"service-api-aka-ingress-v2\">Service API aka Ingress V2</h2><p>Announced at Kubecon NA in 2019 by Google there has been substantial effort in  creating an “Ingress V2” which is now known as the <a href=\"https://github.com/kubernetes-sigs/service-apis\">Service API</a>.</p><p>This specification aims to solve a few problems:</p><ul><li>Provide clean separation and role-based control</li><li>Uplevel the Ingress specification</li><li>Specify standard methods of extending the Ingress specification</li></ul><p>To do so, the specification currently consists of 4 different CRD Ressources:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/bVzQquzU0GRBFEZBpOLTIniju6zwytLAlLcRP52swLC2GF4Vqw4855Uu-OQFVZdPG5bZ-C8kg9EfLb5L21JpXegV6wwkOJnXDh5EaY2do5--ouHYeDfUbARTSOdWCBhpv2jXVWUM\" class=\"kg-image\" alt></figure><h2 id=\"gatewayclass\">Gatewayclass</h2><p>The GatewayClass is meant to be a Cluster Scoped resource, which is meant to represent a “category” of gateways. It’s similar to the former `ingress.class` annotation or the now included IngressClass resource.</p><p>The expected use-case is to have more than one GatewayClass per Ingress Controller provider. These classes may have a variety of default settings, which are inherited by the cluster-level gateway. Also, this can be used to pass additional configuration down to that gateway. Since it is a cluster scoped resource, it's expected to be managed by the Infrastructure provider.</p><!--kg-card-begin: markdown--><pre><code>---\nkind: GatewayClass\nmetadata:\n  name: cluster-gateway\nspec:\n  controller: &quot;acme.io/gateway-controller&quot;\n  parametersRef:\n    apiVersion: core/v1\n    kind: ConfigMap\n    namespace: acme-system\n    name: internet-gateway\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"gateway\">Gateway</h2><p>The gateway has a life-cycle which is tied with the infrastructure. For instance, one Gateway could be running an instance of Traefik or one AWS ELB. As already mentioned, it’s linked to a Gateway class for inferring configuration.</p><p>The gateway sets listener bindings (Address, Ports, TLS…) and the routes served by the gateway.</p><!--kg-card-begin: markdown--><pre><code>---\nkind: Gateway\nname: my-app-gw\nnamespace: my-app\nspec:\n  class: from-internet\n  listeners:\n  - address:\n      ip: 1.2.3.4\n    protocols: [&quot;http&quot;] # implies port 80.\n    routes:\n      ...\n  - address:\n      ip: 1.2.3.4\n    protocols: [&quot;https&quot;] # implies port 443\n    certificates:\n    - name: my-secret\n    - apiGroup: certmanager.k8s.io\n      kind: Certificate               \n      name: lets-encrypt-cert\n    routes:\n      - route:\n        name: http-app-1\n        namespace: app-1\n        kind: HTTPRoute\n</code></pre>\n<!--kg-card-end: markdown--><p>It also has some sane default protocols like: http, https, TCP… which map to predefined ports.</p><h2 id=\"route\">Route</h2><p>Last but not least, a route is used to describe a way to handle traffic given protocol level descriptions.</p><p>A route can be of a different ressource (HTTPRoute, TLSRoute, TCPRoute…) and can therefore have different protocol level descriptions taken into consideration for routing. Each of these Protocols have different attributes which could be used for route matching. It can also be used to delegate to other Route Resources in a multi-tenant scenario, for example, where  one team offers a global authentication service where you would want to forward from within your current scope.</p><!--kg-card-begin: markdown--><pre><code>---\nkind: HTTPRoute\nname: delegate-1\nnamespace: other-team\nrules:\n- match:\n    http:\n      host: bar.com\n      path:\n        prefix: /store\n  action:\n    backend:                           \n      name: delegate-1                  \n      namespace: other-team             \n      kind: HTTPRoute\n      apiGroup: networking.k8s.io\n</code></pre>\n<!--kg-card-end: markdown--><p>In the above HTTPRoute example it’s listening for traffic with a host of bar.com and looking for a Pathprefix of /store, which is close to some of the examples we’ve explored on  traditional Ingress but allows for some additional specifics.</p><h2 id=\"extensibility\">Extensibility</h2><p>Taking what we just learned to the next level, the question that arrives now is: “How can we extend that, for example, to implement Rewrites?”</p><p>For that purpose, the specification currently offers three different levels of support:</p><ul><li>Core</li><li>Extended</li><li>Custom</li></ul><p>Core functionality is guaranteed between all solutions respecting a specific set of API’s. Extended is a standardised API, but functionality is not guaranteed between all solutions.</p><p>For special use-cases, which might be vendor specific, there’s a custom layer where the implementation can be custom built to meet specific requirements. Usually, these will end up in CRD’s or custom annotations.</p><h2 id=\"sumup\">SumUp</h2><p>Service API is a new set of forward-looking API’s attempting to solve some issues that have become apparent over the evolving usage of Ingress. However, as it’s a bit more complex and might not solve all the simple use cases Ingress is capable of solving, it’s not meant to replace Ingress but rather provide an alternative for complex use cases.</p><p>Eventually, Traefik itself will support the new Service API spec. In the meantime, we offer support for both native Kubernetes Ingress and have extended support for Ingress through the use of CRDs. Learn more about <a href=\"https://containous.ghost.io/solutions/kubernetes-ingress/\">how we do both and empower developers with flexible and easy to use Kubernetes Ingress</a>.<br></p>","url":"https://containous.ghost.io/blog/kubernetes-ingress-service-api-demystified/","canonical_url":null,"uuid":"a1c9986a-1406-40fe-9274-993271aed29f","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ec2a072c49e39004576b7ad","reading_time":5}},{"node":{"id":"Ghost__Post__5ec861094e2e9a0045ce7983","title":"Five ways to control access to your applications on Kubernetes","slug":"five-ways-to-control-access-to-your-applications-on-kubernetes","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/05/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/b860b48631b12a5416a73a890556681c/47498/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg","srcSet":"/static/b860b48631b12a5416a73a890556681c/9dc27/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg 300w,\n/static/b860b48631b12a5416a73a890556681c/4fe8c/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg 600w,\n/static/b860b48631b12a5416a73a890556681c/47498/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg 1200w,\n/static/b860b48631b12a5416a73a890556681c/52258/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg 1800w,\n/static/b860b48631b12a5416a73a890556681c/a41d1/5-ways-to-control-access-to-your-applications-on-Kubernetes-1.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"How should developers implement access control, particularly authentication, within the context of k8s?","custom_excerpt":"How should developers implement access control, particularly authentication, within the context of k8s?","visibility":"public","created_at_pretty":"22 May, 2020","published_at_pretty":"May 27, 2020","updated_at_pretty":"16 June, 2020","created_at":"2020-05-22T23:32:25.000+00:00","published_at":"2020-05-27T05:20:11.000+00:00","updated_at":"2020-06-16T14:27:07.000+00:00","meta_title":"Five ways to control access to your applications on Kubernetes","meta_description":"How should developers implement access control, particularly authentication, within the context of k8s?","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/05/5-ways-to-control-access-to-your-applications-on-Kubernetes-Twitter.jpg","twitter_title":null,"authors":[{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"}],"primary_author":{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Development teams have grown adept at leveraging modern programming languages\nand cloud technologies in a bid to increase their productivity and reduce\ndevelopment cycle times. Given the flexibility of the cloud-native ecosystem,\nthese advancements have also expanded the surface area of security-related\nissues such as access-control. While many organizations are adopting Kubernetes\n[https://kubernetes.io/] (K8s) as their platform of choice for deploying and\nmanaging containerized applications, a natural question arises: How should\ndevelopers implement access control, particularly authentication, within the\ncontext of k8s? In this article, we’ll explore this question by covering the\nfollowing topics:\n\n 1. Reviewing common methods for authentication\n 2. Identifying how some of these methods can be readily integrated with\n    Kubernetes\n\nCommon authentication approaches\nLDAP\nMany organizations have historically adopted some form of directory service\nimplementation such as Active Directory (AD) for storing information including\nuser and organizational data. The majority of these systems support the open \nLightweight Directory Access Protocol (LDAP) protocol\n[https://tools.ietf.org/html/rfc4511] standard. By integrating with LDAP\napplications may seamlessly authenticate users within an organization by\nleveraging the existing user information managed by IT. Moreover, LDAP allows\napplications to utilize additional information such as groups and policies to\nenforce access-control. Therefore, integrating with LDAP for both is a\nreasonable option to consider for line-of-business and internal-facing\nworkloads.\n\nOAuth 2.0\nIn the context of third-party web applications, users often find themselves\nhaving to log in to many disparate systems where a central user identity service\nisn’t available. While creating unique accounts for each service is an option,\nthis solution does not scale. The OAuth 2.0 protocol\n[https://tools.ietf.org/html/rfc6749] is one approach that can help solve this\nchallenge, and it is commonly used as part of authentication flow\nimplementations such as the OpenID Connect [https://openid.net/connect/] \nstandard. The main benefit of using OAuth 2.0 is it gives the ability for the\nuser to approve delegated access for applications. This enables users to\nleverage existing identity providers (such as their Google account) to\nauthenticate themselves, allowing control over the information being shared,\nwith third-party applications.\n\nJSON Web Token (JWT)\nJSON web tokens [https://tools.ietf.org/html/rfc7519] are an increasingly\npopular choice for authentication, particularly for APIs. These tokens are\ncomposed of Base64URL [https://base64.guru/standards/base64url] encoded JSON\nobjects. Specifically, the token is constructed by concatenating a header JSON\nobject, payload JSON object, and signature. The cryptographic signature is\ncalculated using a shared secret or public/private key pair and can be used to\nauthenticate the source of the object. For example, given a shared secret, a JWT\nsignature can be computed using the HS256 (HMAC with SHA-256) algorithm as\nfollows:\n\nsignature = HS256(\n    Base64URLEncoding(header) + '.' + Base64URLEncoding(payload),\n    secret\n)\n\n\nThe final JWT is derived by concatenation of the three components:\n\nBase64URLEncoding(header) + '.' + Base64URLEncoding(payload) + '.' + signature\n\n\nIt’s worth noting these tokens won’t provide data security as they aren’t\nencrypted but their straight forward approach can be used by APIs to identify\ncallers. JWTs are a good option when integrating authentication providers with\nuser-facing APIs and inter-service communications, and are often implemented\nutilizing OAuth 2.0 flows [https://oauth.net/2/jwt/].\n\nHMAC\nThe JWT example above highlights the use of the HMAC with SHA-256 algorithm\n[https://tools.ietf.org/html/rfc2104.html] when computing the signature of the\ntoken header and payload. The use of HMACs can be generalized in that a\nsignature for any payload can be generated using any number of specifications.\nThe JWT is a special case where the payload happens to be a JSON object but the\nsame mechanism can be used with other data to achieve the objective of\nauthenticating a signed message.\n\nMutual TLS\nTLS authentication [https://en.wikipedia.org/wiki/Transport_Layer_Security] in\nthe context of web applications is fairly ubiquitous these days. The general\nidea is that certificates are used to authenticate a website (or web service) so\nthe client can be confident that the server is who it claims to be. Mutual TLS\nextends this model to be bidirectional. Not only does the client authenticate\nthe server identity, but the server confirms the identity of the client so that\nit may enforce access control and authorization policies. Mutual TLS\n[https://en.wikipedia.org/wiki/Mutual_authentication] is commonly deployed as\npart of inter-service and business-to-business communications where there are a\nlimited and known set of clients that are designed to access common endpoints.\n\nIntegrating application authentication with Kubernetes\nGiven our brief review of common authentication approaches, let’s turn to the\nquestion of how they can be incorporated when applications are deployed on\nKubernetes. As is often the case with K8s, there is a high degree of flexibility\nto implement a solution that meets the needs of individual organizations. Let’s\nreview a couple of important patterns that can be adopted for most use cases.\n\nIngress Controllers\nIngress Controllers are the most common mechanism used today when connecting\nusers to applications in Kubernetes\n[/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/].\nThere are many considerations to make when selecting a specific controller, as\neach is often implemented differently and based upon different underlying\ntechnology. Among the many decision criteria when comparing controllers\n[/blog/13-key-considerations-when-selecting-an-ingress-controller-for-kubernetes-d3e5d98ed8b7/]\n, one must consider how well candidates can support the relevant subset of\nauthentication approaches we’ve highlighted.\n\nTraefik [/traefik/] comes with a built-in ForwardAuth middleware\n[https://docs.traefik.io/middlewares/forwardauth/] feature that can be used to\ndelegate authentication to an external service. By integrating this capability\nas part of your K8s Ingress strategy, all services exposed in the traffic flow\nobtain the benefits of authentication management without incurring the\ncomplexity at the individual service layer. TraefikEE [/traefikee/] simplifies\nthe management of auth providers even further by integrating support for LDAP,\nOAuth 2.0, JWT, and HMAC all within a unified solution.\n\nAuthentication servers\nThe pattern of integrating authentication capabilities through an Ingress\nController can be extended even further by employing dedicated authentication\nservers into the architecture. Authelia [https://github.com/authelia/authelia] \nis an example of an open-source authentication and authorization server which\nworks with K8s and has been successfully integrated with Ingress technologies\nsuch as Traefik. In addition to the mechanisms covered earlier, these\nfunction-specific services can provide advanced authentication capabilities such\nas 2FA [https://en.wikipedia.org/wiki/Multi-factor_authentication] and SSO\n[https://en.wikipedia.org/wiki/Single_sign-on].\n\nConclusion\nControlling application access through authentication is an important\nconsideration in any enterprise scenario, and its importance is only amplified\nwhen adopting Kubernetes. By thoughtfully selecting architectural patterns and\ntechnologies, users can easily integrate their choice of best-practice\nauthentication approaches with minimal additional effort.\n\nThere has never been a better time than now to get started with Traefik in\nKubernetes. Our open-source Traefik edition\n[https://github.com/containous/traefik] with the support of a large and active\ncommunity is available for free and includes support for the most recent\nadvancements in Kubernetes Ingress technology. If you’re an enterprise looking\nto implement the most popular cloud-native load balancer with commercial\nsupport, high-availability, and authentication modules already built-in you\nshould schedule a demo [https://info.containo.us/request-demo-traefikee] with\nour sales team and learn more about how we make networking boring.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/05/5-ways-to-control-access-to-your-applications-on-Kubernetes.jpg\" class=\"kg-image\" alt=\"Kubernetes Authentication\"></figure><!--kg-card-begin: markdown--><p>Development teams have grown adept at leveraging modern programming languages and cloud technologies in a bid to increase their productivity and reduce development cycle times. Given the flexibility of the cloud-native ecosystem, these advancements have also expanded the surface area of security-related issues such as access-control. While many organizations are adopting <a href=\"https://kubernetes.io/\">Kubernetes</a> (K8s) as their platform of choice for deploying and managing containerized applications, a natural question arises: How should developers implement access control, particularly authentication, within the context of k8s? In this article, we’ll explore this question by covering the following topics:</p>\n<ol>\n<li>Reviewing common methods for authentication</li>\n<li>Identifying how some of these methods can be readily integrated with Kubernetes</li>\n</ol>\n<!--kg-card-end: markdown--><h2 id=\"common-authentication-approaches\">Common authentication approaches</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/05/Common-authentication-approaches@1x.jpg\" class=\"kg-image\" alt=\"Common Authentication Approach\"></figure><!--kg-card-begin: markdown--><h3 id=\"ldap\">LDAP</h3>\n<p>Many organizations have historically adopted some form of directory service implementation such as Active Directory (AD) for storing information including user and organizational data. The majority of these systems support the open <a href=\"https://tools.ietf.org/html/rfc4511\" target=\"_blank\" rel=\"nofollow\">Lightweight Directory Access Protocol (LDAP) protocol</a> standard. By integrating with LDAP applications may seamlessly authenticate users within an organization by leveraging the existing user information managed by IT. Moreover, LDAP allows applications to utilize additional information such as groups and policies to enforce access-control. Therefore, integrating with LDAP for both is a reasonable option to consider for line-of-business and internal-facing workloads.</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id=\"oauth20\">OAuth 2.0</h3>\n<p>In the context of third-party web applications, users often find themselves having to log in to many disparate systems where a central user identity service isn’t available. While creating unique accounts for each service is an option, this solution does not scale. The <a href=\"https://tools.ietf.org/html/rfc6749\" target=\"_blank\" rel=\"nofollow\">OAuth 2.0 protocol</a> is one approach that can help solve this challenge, and it is commonly used as part of authentication flow implementations such as the <a href=\"https://openid.net/connect/\" target=\"_blank\" rel=\"nofollow\">OpenID Connect</a> standard. The main benefit of using OAuth 2.0 is it gives the ability for the user to approve delegated access for applications. This enables users to leverage existing identity providers (such as their Google account) to authenticate themselves, allowing control over the information being shared, with third-party applications.</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id=\"jsonwebtokenjwt\">JSON Web Token (JWT)</h3>\n<p><a href=\"https://tools.ietf.org/html/rfc7519\" target=\"_blank\" rel=\"nofollow\">JSON web tokens</a> are an increasingly popular choice for authentication, particularly for APIs. These tokens are composed of <a href=\"https://base64.guru/standards/base64url\" target=\"_blank\" rel=\"nofollow\">Base64URL</a> encoded JSON objects. Specifically, the token is constructed by concatenating a header JSON object, payload JSON object, and signature. The cryptographic signature is calculated using a shared secret or public/private key pair and can be used to authenticate the source of the object. For example, given a shared secret, a JWT signature can be computed using the HS256 (HMAC with SHA-256) algorithm as follows:</p>\n<pre><code>signature = HS256(\n    Base64URLEncoding(header) + '.' + Base64URLEncoding(payload),\n    secret\n)\n</code></pre>\n<p>The final JWT is derived by concatenation of the three components:</p>\n<pre><code>Base64URLEncoding(header) + '.' + Base64URLEncoding(payload) + '.' + signature\n</code></pre>\n<p>It’s worth noting these tokens won’t provide data security as they aren’t encrypted but their straight forward approach can be used by APIs to identify callers. JWTs are a good option when integrating authentication providers with user-facing APIs and inter-service communications, and are often implemented utilizing <a href=\"https://oauth.net/2/jwt/\" target=\"_blank\" rel=\"nofollow\">OAuth 2.0 flows</a>.</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id=\"hmac\">HMAC</h3>\n<p>The JWT example above highlights the use of the <a href=\"https://tools.ietf.org/html/rfc2104.html\" target=\"_blank\" rel=\"nofollow\">HMAC with SHA-256 algorithm</a> when computing the signature of the token header and payload. The use of HMACs can be generalized in that a signature for any payload can be generated using any number of specifications. The JWT is a special case where the payload happens to be a JSON object but the same mechanism can be used with other data to achieve the objective of authenticating a signed message.</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h3 id=\"mutualtls\">Mutual TLS</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Transport_Layer_Security\" target=\"_blank\" rel=\"nofollow\">TLS authentication</a> in the context of web applications is fairly ubiquitous these days. The general idea is that certificates are used to authenticate a website (or web service) so the client can be confident that the server is who it claims to be. Mutual TLS extends this model to be bidirectional. Not only does the client authenticate the server identity, but the server confirms the identity of the client so that it may enforce access control and authorization policies. <a href=\"https://en.wikipedia.org/wiki/Mutual_authentication\" target=\"_blank\" rel=\"nofollow\">Mutual TLS</a> is commonly deployed as part of inter-service and business-to-business communications where there are a limited and known set of clients that are designed to access common endpoints.</p>\n<!--kg-card-end: markdown--><h2 id=\"integrating-application-authentication-with-kubernetes\">Integrating application authentication with Kubernetes</h2><p>Given our brief review of common authentication approaches, let’s turn to the question of how they can be incorporated when applications are deployed on Kubernetes. As is often the case with K8s, there is a high degree of flexibility to implement a solution that meets the needs of individual organizations. Let’s review a couple of important patterns that can be adopted for most use cases.</p><h3 id=\"ingress-controllers\">Ingress Controllers</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/05/Traefik-Ingress-Controller.jpg\" class=\"kg-image\" alt=\"Ingress Controller with Traefik\"></figure><!--kg-card-begin: markdown--><p>Ingress Controllers are the most common mechanism used today when <a href=\"https://containous.ghost.io/blog/connecting-users-to-applications-with-kubernetes-ingress-controllers/\">connecting users to applications in Kubernetes</a>. There are many considerations to make when selecting a specific controller, as each is often implemented differently and based upon different underlying technology. Among the many <a href=\"https://containous.ghost.io/blog/13-key-considerations-when-selecting-an-ingress-controller-for-kubernetes-d3e5d98ed8b7/\">decision criteria when comparing controllers</a>, one must consider how well candidates can support the relevant subset of authentication approaches we’ve highlighted.</p>\n<p><a href=\"https://containous.ghost.io/traefik/\">Traefik</a> comes with a built-in <a href=\"https://docs.traefik.io/middlewares/forwardauth/\">ForwardAuth middleware</a> feature that can be used to delegate authentication to an external service. By integrating this capability as part of your K8s Ingress strategy, all services exposed in the traffic flow obtain the benefits of authentication management without incurring the complexity at the individual service layer. <a href=\"https://containous.ghost.io/traefikee/\">TraefikEE</a> simplifies the management of auth providers even further by integrating support for LDAP, OAuth 2.0, JWT, and HMAC all within a unified solution.</p>\n<h3 id=\"authenticationservers\">Authentication servers</h3>\n<p>The pattern of integrating authentication capabilities through an Ingress Controller can be extended even further by employing dedicated authentication servers into the architecture. <a href=\"https://github.com/authelia/authelia\" target=\"_blank\" rel=\"nofollow\">Authelia</a> is an example of an open-source authentication and authorization server which works with K8s and has been successfully integrated with Ingress technologies such as Traefik. In addition to the mechanisms covered earlier, these function-specific services can provide advanced authentication capabilities such as <a href=\"https://en.wikipedia.org/wiki/Multi-factor_authentication\" target=\"_blank\" rel=\"nofollow\">2FA</a> and <a href=\"https://en.wikipedia.org/wiki/Single_sign-on\" target=\"_blank\" rel=\"nofollow\">SSO</a>.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>Controlling application access through authentication is an important consideration in any enterprise scenario, and its importance is only amplified when adopting Kubernetes. By thoughtfully selecting architectural patterns and technologies, users can easily integrate their choice of best-practice authentication approaches with minimal additional effort.</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/05/TraefikEE-Ingress-Controller.jpg\" class=\"kg-image\" alt=\"Ingress Controller with TraefikEE\"></figure><!--kg-card-begin: markdown--><p>There has never been a better time than now to get started with Traefik in Kubernetes. Our <a href=\"https://github.com/containous/traefik\">open-source Traefik edition</a> with the support of a large and active community is available for free and includes support for the most recent advancements in Kubernetes Ingress technology. If you’re an enterprise looking to implement the most popular cloud-native load balancer with commercial support, high-availability, and authentication modules already built-in you should <a href=\"https://info.containo.us/request-demo-traefikee\" target=\"_blank\" rel=\"nofollow\">schedule a demo</a> with our sales team and learn more about how we make networking boring.</p>\n<!--kg-card-end: markdown-->","url":"https://containous.ghost.io/blog/five-ways-to-control-access-to-your-applications-on-kubernetes/","canonical_url":null,"uuid":"26e84f80-3637-4600-82da-292eb70149e2","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ec861094e2e9a0045ce7983","reading_time":5}},{"node":{"id":"Ghost__Post__5eb58b45c49e39004576b610","title":"Combining Ingress Controllers and External Load Balancers with Kubernetes","slug":"combining-ingress-controllers-and-external-load-balancers-with-kubernetes","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/05/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-containous-2.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/9b55622b4567a60fc818e7ec45f13481/47498/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-containous-2.jpg","srcSet":"/static/9b55622b4567a60fc818e7ec45f13481/9dc27/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-containous-2.jpg 300w,\n/static/9b55622b4567a60fc818e7ec45f13481/4fe8c/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-containous-2.jpg 600w,\n/static/9b55622b4567a60fc818e7ec45f13481/47498/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-containous-2.jpg 1200w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"The great promise of Kubernetes is the ability to easily deploy and scale containerized applications. How Load Balancers work together with the Ingress Controllers in a Kubernetes architecture?","custom_excerpt":"The great promise of Kubernetes is the ability to easily deploy and scale containerized applications. How Load Balancers work together with the Ingress Controllers in a Kubernetes architecture?","visibility":"public","created_at_pretty":"08 May, 2020","published_at_pretty":"May 14, 2020","updated_at_pretty":"09 June, 2020","created_at":"2020-05-08T16:39:33.000+00:00","published_at":"2020-05-14T05:05:21.000+00:00","updated_at":"2020-06-09T05:37:52.000+00:00","meta_title":"Combining Ingress Controllers & External Load Balancers with Kubernetes","meta_description":"The great promise of Kubernetes is the ability to easily deploy and scale containerized applications. How Load Balancers work together with the Ingress Controllers in a Kubernetes architecture?","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/05/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes-Twitter.jpg","twitter_title":null,"authors":[{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"}],"primary_author":{"name":"Kevin Crawley","slug":"kevincrawley","bio":"Kevin is a Developer Advocate at Containous, where he contributes to the team by bringing his passion and experience for developer productivity and automation.","profile_image":"https://containous.ghost.io/content/images/2020/04/2020-03-24_14-04-57.png","twitter":"@notsureifkevin","facebook":null,"website":"https://containo.us"},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"#kubernetes-ingress-related-resource","slug":"hash-kubernetes-ingress-related-resource","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"internal"}],"plaintext":"The great promise of Kubernetes [https://kubernetes.io/] (k8s) is the ability to\neasily deploy and scale containerized applications. By automating the process of\nallocating and provisioning compute and storage resources for Pods across nodes,\nk8s reduces the operational complexity of day-to-day operations. However,\norchestrating containers alone doesn’t necessarily help engineers meet the\nconnectivity requirements for users. Specifically, a Kubernetes Deployment\nconfigures Pods with private IP addresses and precludes incoming traffic over\nthe network. Outside of Kubernetes, operators are typically familiar with\ndeploying external load balancers, either in cloud or physical data center\nenvironments, to route traffic to application instances. However, effectively\nmapping these operational patterns to k8s requires understanding how Load\nBalancers [https://kubernetes.io/docs/concepts/services-networking/] work\ntogether with the Ingress Controllers\n[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/] \nin a Kubernetes architecture.\n\nExternal load balancers and Kubernetes\nOverview of external LBs and K8s\nIn order to expose application endpoints, Kubernetes networking allows users to\nexplicitly define Services\n[https://kubernetes.io/docs/concepts/services-networking/service/]. K8s then\nautomates provisioning appropriate networking resources based upon the service\ntype specified. The NodePort service type exposes an allocated port that can be\naccessed over the network on each node in the k8s cluster. The LoadBalancer\nservice type uses this same mechanism to deploy and configure an external load\nbalancer (often through a cloud-managed API) which forwards traffic to an\napplication using the NodePort. When a request is routed to the configured port\non a node, it forwards packets as needed to direct traffic to the destination\nPods using kube-proxy.\n\nFig. a: External load balancer defined for each applicationGiven the ability to\neasily define LoadBalancer services to route incoming traffic from external\nclients, the story may seemingly sound complete. However, in any real-life\nscenario, directly using external load balancers for every application (fig. a) \nthat needs external access has significant drawbacks including:\n\n * Cost overheads: External load balancers, whether cloud-managed or\n   instantiated through physical network appliances on-premises, can be\n   expensive. For k8s clusters with many applications, these costs will quickly\n   add up.\n * Operational complexity: Load balancers require various resources (IP\n   addresses, DNS, certificates, etc.) that can be painful to manage,\n   particularly in highly dynamic environments where there may be transient\n   endpoints for staging and development deployments.\n * Monitoring and logging: Given the importance of external load balancers in\n   the traffic flow, being able to effectively centralize monitoring and logging\n   data is critical. However, this may become challenging in practice when there\n   are multiple load balancers involved.\n\nThe perfect marriage: Load balancers and Ingress Controllers\nIt’s clear that external load balancers alone aren’t a practical solution for\nproviding the networking capabilities necessary for a k8s environment. Luckily,\nthe Kubernetes architecture allows users to combine load balancers with an\nIngress Controller. The core concepts are as follows: instead of provisioning an\nexternal load balancer for every application service that needs external\nconnectivity, users deploy and configure a single load balancer that targets an\nIngress Controller. The Ingress Controller serves as a single entrypoint and can\nthen route traffic to multiple applications in the cluster. Key elements of this\napproach include:\n\n * Ingress Controllers are k8s applications: While they may seem somewhat\n   magical, it’s useful to keep in mind that Ingress Controllers are nothing\n   more than standard Kubernetes applications instantiated via Pods.\n * Ingress Controllers are exposed as a service: The k8s application that\n   constitutes an Ingress Controller is exposed through a LoadBalancer service\n   type thereby mapping it to an external load balancer.\n * Ingress Controllers route to underlying applications using ClusterIPs: As an\n   intermediary between the external load balancer and applications, the Ingress\n   Controller uses ClusterIP service types to route and balance traffic across\n   application instances based upon operator-provided configurations.\n\nFig. b: Dynamic load balancing through ingressInjecting the Ingress Controller\nin the traffic path allows users to gain the benefits of external load balancer\ncapabilities while avoiding the pitfalls of relying upon them exclusively (fig.\nb). Indeed, the Kubernetes architecture allows operators to integrate multiple\nIngress Controllers, thereby providing a high degree of flexibility to meet\nspecific requirements. This can be a bit overwhelming for those new to\nKubernetes networking, so let’s review a few example deployment patterns.\n\nExample deployment patterns\nLoad balancer with NodePorts and traffic forwarding\nAs mentioned earlier, a LoadBalancer service type results in an external load\nbalancer that uses NodePorts to reach backend services. Since every node exposes\nthe target port, the load balancer can spread traffic across all of them.\nHowever, the underlying Pods may only be running on a subset of the k8s nodes\ncreating the potential need for traffic forwarding. As part of forwarding\ntraffic, the kube-proxy performs source network address translation (SNAT).\nWhile this obfuscates the source IP address of requestors, the use of HTTP\nheaders such as X-Forwarded-For (and its standardized variant Forwarded) can be\nutilized where business requirements require it (e.g. for compliance purposes).\n\nLoad balancer with NodePorts and no SNAT\nTo avoid SNAT in the traffic flow, we can modify the previous deployment pattern\nto force the external load balancer to only target nodes that reflect Pods\nrunning the Ingress Controller deployment. Specifically, the\nexternalTrafficPolicy on the LoadBalancer service for the Ingress Controller can\nbe set to Local instead of the default Cluster value. Some potential drawbacks\nof this pattern, however, include:\n\n * Reduced load-spreading: Since all traffic is funneled to a subset of nodes,\n   there may be traffic imbalances\n * Use of health checks: The external load balancer uses a health check to\n   determine which nodes to target, and these can result in transient errors\n   (e.g. due to rolling deployments, health check timeouts, etc.)\n\nMultiple external LBs and Ingress Controllers: Craft a solution that meets your\nneeds\nAs a final deployment pattern, we can create advanced configurations consisting\nof multiple external load balancers each of which map to a different Ingress\nController. Users can select a technology-specific Ingress Controller based upon\ndesired feature capabilities (e.g. NGINX or Traefik), and configure Ingress\nresources for underlying applications accordingly. Examples of potential\ncriteria\n[/blog/13-key-considerations-when-selecting-an-ingress-controller-for-kubernetes-d3e5d98ed8b7/] \nthat may be relevant when comparing candidate controllers include:\n\n * Protocol support: If your needs extend beyond just HTTP(S) and may require\n   routing TCP/UDP or gRPC, it’s important to recognize that not all controller\n   implementations support the full array of protocols\n * Zero-downtime configuration updates: Not all controllers support\n   configuration updates without incurring downtime\n * High availability (HA): If you want to avoid your ingress controller becoming\n   a potential single point of failure (SPOF) for external traffic, you should\n   identify controllers that support HA configurations\n * Enterprise support: While many open source controller options are available,\n   not all provide enterprise support options that teams can rely on when needed \n\nThe aspects of previous deployment patterns such as configuring\nexternalTrafficPolicy to allow for load spreading or avoiding SNAT can be\nincorporated, with each Ingress Controller potentially configured differently.\nThe ability to easily piece together external load balancers and Ingress\nControllers in a manner that meets the unique business needs of an organization\nexemplifies the benefits that the flexibility of k8s networking can provide.\n\nTo learn more, check out this video\n[https://info.containo.us/webinar-deploying-external-load-balancers-in-kubernetes] \nwe recorded recently that further explains Kubernetes Ingress and the different\npatterns for external load balancers in k8s\n[https://info.containo.us/webinar-deploying-external-load-balancers-in-kubernetes]\n.","html":"<figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/05/Combining-Ingress-Controllers-and-External-Load-Balancers-with-Kubernetes.jpg\" class=\"kg-image\"></figure><p>The great promise of <a href=\"https://kubernetes.io/\">Kubernetes</a> (k8s) is the ability to easily deploy and scale containerized applications. By automating the process of allocating and provisioning compute and storage resources for Pods across nodes, k8s reduces the operational complexity of day-to-day operations. However, orchestrating containers alone doesn’t necessarily help engineers meet the connectivity requirements for users. Specifically, a Kubernetes Deployment configures Pods with private IP addresses and precludes incoming traffic over the network. Outside of Kubernetes, operators are typically familiar with deploying external load balancers, either in cloud or physical data center environments, to route traffic to application instances. However, effectively mapping these operational patterns to k8s requires understanding how <a href=\"https://kubernetes.io/docs/concepts/services-networking/\">Load Balancers</a> work together with the <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress Controllers</a> in a Kubernetes architecture.</p><h2 id=\"external-load-balancers-and-kubernetes\">External load balancers and Kubernetes</h2><h3 id=\"overview-of-external-lbs-and-k8s\">Overview of external LBs and K8s</h3><p>In order to expose application endpoints, Kubernetes networking allows users to explicitly define <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Services</a>. K8s then automates provisioning appropriate networking resources based upon the service type specified. The NodePort service type exposes an allocated port that can be accessed over the network on each node in the k8s cluster. The LoadBalancer service type uses this same mechanism to deploy and configure an external load balancer (often through a cloud-managed API) which forwards traffic to an application using the NodePort. When a request is routed to the configured port on a node, it forwards packets as needed to direct traffic to the destination Pods using kube-proxy.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/05/services_loadbalancer.png\" class=\"kg-image\" alt=\"External-load-balancer\"><figcaption><strong>Fig. a</strong>: External load balancer defined for each application</figcaption></figure><p>Given the ability to easily define LoadBalancer services to route incoming traffic from external clients, the story may seemingly sound complete. However, in any real-life scenario, directly using external load balancers for every application <em>(fig. a)</em> that needs external access has significant drawbacks including:</p><ul><li><strong>Cost overheads</strong>: External load balancers, whether cloud-managed or instantiated through physical network appliances on-premises, can be expensive. For k8s clusters with many applications, these costs will quickly add up.</li><li><strong>Operational complexity</strong>: Load balancers require various resources (IP addresses, DNS, certificates, etc.) that can be painful to manage, particularly in highly dynamic environments where there may be transient endpoints for staging and development deployments.</li><li><strong>Monitoring and logging</strong>: Given the importance of external load balancers in the traffic flow, being able to effectively centralize monitoring and logging data is critical. However, this may become challenging in practice when there are multiple load balancers involved.</li></ul><h2 id=\"the-perfect-marriage-load-balancers-and-ingress-controllers\">The perfect marriage: Load balancers and Ingress Controllers</h2><p>It’s clear that external load balancers alone aren’t a practical solution for providing the networking capabilities necessary for a k8s environment. Luckily, the Kubernetes architecture allows users to combine load balancers with an Ingress Controller. The core concepts are as follows: instead of provisioning an external load balancer for every application service that needs external connectivity, users deploy and configure a single load balancer that targets an Ingress Controller. The Ingress Controller serves as a single entrypoint and can then route traffic to multiple applications in the cluster. Key elements of this approach include:</p><ul><li><strong>Ingress Controllers are k8s applications:</strong> While they may seem somewhat magical, it’s useful to keep in mind that Ingress Controllers are nothing more than standard Kubernetes applications instantiated via Pods.</li><li><strong>Ingress Controllers are exposed as a service:</strong> The k8s application that constitutes an Ingress Controller is exposed through a LoadBalancer service type thereby mapping it to an external load balancer.</li><li><strong>Ingress Controllers route to underlying applications using ClusterIPs</strong>: As an intermediary between the external load balancer and applications, the Ingress Controller uses ClusterIP service types to route and balance traffic across application instances based upon operator-provided configurations.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://containous.ghost.io/content/images/2020/05/ingress_have_services_too.png\" class=\"kg-image\" alt=\"Dynamic-load-balancing-through-ingress\"><figcaption><strong>Fig. b</strong>: Dynamic load balancing through ingress</figcaption></figure><p>Injecting the Ingress Controller in the traffic path allows users to gain the benefits of external load balancer capabilities while avoiding the pitfalls of relying upon them exclusively <em>(fig. b)</em>. Indeed, the Kubernetes architecture allows operators to integrate multiple Ingress Controllers, thereby providing a high degree of flexibility to meet specific requirements. This can be a bit overwhelming for those new to Kubernetes networking, so let’s review a few example deployment patterns.</p><h2 id=\"example-deployment-patterns\">Example deployment patterns</h2><h3 id=\"load-balancer-with-nodeports-and-traffic-forwarding\">Load balancer with NodePorts and traffic forwarding</h3><p>As mentioned earlier, a LoadBalancer service type results in an external load balancer that uses NodePorts to reach backend services. Since every node exposes the target port, the load balancer can spread traffic across all of them. However, the underlying Pods may only be running on a subset of the k8s nodes creating the potential need for traffic forwarding. As part of forwarding traffic, the kube-proxy performs source network address translation (SNAT). While this obfuscates the source IP address of requestors, the use of HTTP headers such as X-Forwarded-For (and its standardized variant Forwarded) can be utilized where business requirements require it (e.g. for compliance purposes).</p><h3 id=\"load-balancer-with-nodeports-and-no-snat\">Load balancer with NodePorts and no SNAT</h3><p>To avoid SNAT in the traffic flow, we can modify the previous deployment pattern to force the external load balancer to only target nodes that reflect Pods running the Ingress Controller deployment. Specifically, the externalTrafficPolicy on the LoadBalancer service for the Ingress Controller can be set to Local instead of the default Cluster value. Some potential drawbacks of this pattern, however, include:</p><ul><li><strong>Reduced load-spreading:</strong> Since all traffic is funneled to a subset of nodes, there may be traffic imbalances</li><li><strong>Use of health checks:</strong> The external load balancer uses a health check to determine which nodes to target, and these can result in transient errors (e.g. due to rolling deployments, health check timeouts, etc.)</li></ul><h3 id=\"multiple-external-lbs-and-ingress-controllers-craft-a-solution-that-meets-your-needs\">Multiple external LBs and Ingress Controllers: Craft a solution that meets your needs</h3><p>As a final deployment pattern, we can create advanced configurations consisting of multiple external load balancers each of which map to a different Ingress Controller. Users can select a technology-specific Ingress Controller based upon desired feature capabilities (e.g. NGINX or Traefik), and configure Ingress resources for underlying applications accordingly. <a href=\"https://containous.ghost.io/blog/13-key-considerations-when-selecting-an-ingress-controller-for-kubernetes-d3e5d98ed8b7/\">Examples of potential criteria</a> that may be relevant when comparing candidate controllers include:</p><ul><li><strong>Protocol support</strong>: If your needs extend beyond just HTTP(S) and may require routing TCP/UDP or gRPC, it’s important to recognize that not all controller implementations support the full array of protocols</li><li><strong>Zero-downtime configuration updates</strong>: Not all controllers support configuration updates without incurring downtime</li><li><strong>High availability (HA)</strong>: If you want to avoid your ingress controller becoming a potential single point of failure (SPOF) for external traffic, you should identify controllers that support HA configurations</li><li><strong>Enterprise support</strong>: While many open source controller options are available, not all provide enterprise support options that teams can rely on when needed </li></ul><p>The aspects of previous deployment patterns such as configuring externalTrafficPolicy to allow for load spreading or avoiding SNAT can be incorporated, with each Ingress Controller potentially configured differently. The ability to easily piece together external load balancers and Ingress Controllers in a manner that meets the unique business needs of an organization exemplifies the benefits that the flexibility of k8s networking can provide.</p><p>To learn more, <strong><a href=\"https://info.containo.us/webinar-deploying-external-load-balancers-in-kubernetes\">check out this video</a></strong> we recorded recently that further explains <strong><a href=\"https://info.containo.us/webinar-deploying-external-load-balancers-in-kubernetes\">Kubernetes Ingress and the different patterns for external load balancers in k8s</a></strong>.<br></p>","url":"https://containous.ghost.io/blog/combining-ingress-controllers-and-external-load-balancers-with-kubernetes/","canonical_url":null,"uuid":"8e267865-103f-4587-bfe2-78d34b1a16c6","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb58b45c49e39004576b610","reading_time":5}}]}},"pageContext":{"slug":"kubernetes","limit":9,"skip":0,"numberOfPages":2,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/kubernetes/page/2/"}},"staticQueryHashes":["1274566015","2561578252","2731221146","394248586","4145280475","749840385"]}