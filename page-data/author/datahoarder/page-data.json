{"componentChunkName":"component---src-templates-author-tsx","path":"/author/datahoarder/","result":{"data":{"ghostAuthor":{"slug":"datahoarder","name":"Alexander Dmitriev","bio":"I am passionate about recommendation systems and personalization, love to think more about the product than the code, know that weeks of coding saves hours of planning.","cover_image":null,"profile_image":"https://containous.ghost.io/content/images/2020/06/Screen-Shot-2020-06-20-at-21.27.08.png","location":"Moscow","website":"https://github.com/uSasha","twitter":null,"facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5efb70c61555240039b0bf3d","title":"Do Machines Learn? Testing in Production with Traefik","slug":"do-machines-learn-testing-in-production-with-traefik","featured":false,"feature_image":"https://containous.ghost.io/content/images/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg","featureImageSharp":{"childImageSharp":{"fluid":{"src":"/static/1a0b7b5f2141923692e66f755823a69d/47498/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg","srcSet":"/static/1a0b7b5f2141923692e66f755823a69d/9dc27/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg 300w,\n/static/1a0b7b5f2141923692e66f755823a69d/4fe8c/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg 600w,\n/static/1a0b7b5f2141923692e66f755823a69d/47498/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg 1200w,\n/static/1a0b7b5f2141923692e66f755823a69d/52258/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg 1800w,\n/static/1a0b7b5f2141923692e66f755823a69d/a41d1/Do-Machines-Learn--Testing-in-Production-with-Traefik-1.jpg 2000w","sizes":"(max-width: 1200px) 100vw, 1200px"}}},"excerpt":"Here is the story about the personalization of a large-scale book reading service and building the infrastructure to evaluate new models utilizing Traefik.","custom_excerpt":"Here is the story about the personalization of a large-scale book reading service and building the infrastructure to evaluate new models utilizing Traefik.","visibility":"public","created_at_pretty":"30 June, 2020","published_at_pretty":"July 28, 2020","updated_at_pretty":"25 August, 2020","created_at":"2020-06-30T17:05:10.000+00:00","published_at":"2020-07-28T14:30:00.000+00:00","updated_at":"2020-08-25T22:48:13.000+00:00","meta_title":"Do Machines Learn? Testing in Production with Traefik","meta_description":"Here is the story about the personalization of a large-scale book reading service and building the infrastructure to evaluate new models utilizing Traefik.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":"https://containous.ghost.io/content/images/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik-Twitter.png","twitter_title":null,"authors":[{"name":"Alexander Dmitriev","slug":"datahoarder","bio":"I am passionate about recommendation systems and personalization, love to think more about the product than the code, know that weeks of coding saves hours of planning.","profile_image":"https://containous.ghost.io/content/images/2020/06/Screen-Shot-2020-06-20-at-21.27.08.png","twitter":null,"facebook":null,"website":"https://github.com/uSasha"}],"primary_author":{"name":"Alexander Dmitriev","slug":"datahoarder","bio":"I am passionate about recommendation systems and personalization, love to think more about the product than the code, know that weeks of coding saves hours of planning.","profile_image":"https://containous.ghost.io/content/images/2020/06/Screen-Shot-2020-06-20-at-21.27.08.png","twitter":null,"facebook":null,"website":"https://github.com/uSasha"},"primary_tag":{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Blog","slug":"blog","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"How To","slug":"how-to","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"#community-related-resource","slug":"hash-community-related-resource","description":null,"feature_image":null,"featureImageSharp":null,"meta_description":null,"meta_title":null,"visibility":"internal"}],"plaintext":"Guest post by Alexander Dmitriev, Traefik Ambassador\n\nAnyone who runs machine learning models in production while trying to improve\nmodel performance most likely knows about A/B tests\n[https://en.wikipedia.org/wiki/A/B_testing]. But what if you are the only\nmachine learning engineer on the project? You must automate these tests as much\nas possible. And what if you have to make dozens of iterations per year and\nsometimes deploy new versions several times a week? Here is my story about the\npersonalization of a large-scale book reading service and building the\ninfrastructure to evaluate new models utilizing Traefik.\n\nAbout MyBook\nMyBook [https://mybook.ru/] is a subscription reading service that publishes\nbook apps for multiple platforms while striving to provide the best reading\nexperience for our users. We've collected a huge catalog of books, audiobooks,\nratings, and summaries that are conveniently always in your pocket. Bookmarks\nare synced across all our userâ€™s devices and text-audiobook versions. Avid\nreaders can use a subscription with unlimited reading, and those who want just\none book can rent it for half price. My name is Alexander Dmitriev, and I am a\nmachine learning engineer. My field of interest is personalization: recommender\nsystems [https://en.wikipedia.org/wiki/Recommender_system], user satisfaction\nmeasurement [https://en.wikipedia.org/wiki/Computer_user_satisfaction], and\ninfrastructure for these systems. I love to solve problems end-to-end: discuss\nthe user interface with the product manager, prepare data, train machine\nlearning models, write the service, deploy to production, and support it.\n\nBuilding a Smart(er) Recommendation Engine\nHelping customers find books that are interesting to them is essential for a\nretail service like ours. MyBook already has talented editors curating popular\nbook selections, so the next logical step was adding frequently updated personal\nrecommendations. When I started at MyBook, my goal was to build a recommender\nsystem that discovered and suggested books that a customer might find\ninteresting based on their behaviors. This service is known internally as recsys\n, and I'll be focusing on the evolution, automation, and maturation of that\nsystem in this post.\n\nProduction testing is critical for the development of machine learning systems\nlike recsys. Simulating ephemeral properties such as serendipity, variety, and\nrelevance is not feasible with more traditional approaches to testing such as\nunit or integration methodologies used during software development.\n\nWhen the first version of the recommender service was ready, I added a Flask\nserver, built a Docker container, and deployed it on a newly purchased cloud\nserver. Backend engineers added NGINX as a reverse proxy with SSL support\nbecause it had been used for many years on our production systems. The recsys \napplication Flask port was published to the host, and all requests were\nforwarded.\n\nSo, at first, the architecture looked like this:\n\nThe Growing Pains of Productionized Testing\nWe regularly performed A/B tests where we measured the mean number\n[https://en.wikipedia.org/wiki/Mean] of books our users added to their\nbookshelves during a single session. The end results showed us that personalized\nsets performed 30% better than a fixed set of bestsellers handpicked by our\neditors. Eventually, all new versions of recsys rolled out through the A/B test\nagainst the previous one in production. And it looked something like this:\n\nAs time passed and new services appeared, the situation became messy and\nunmanageable:\n\n * Many of the recsys services ports were hardcoded on the backend\n * Prior to each A/B test the health checks, Prometheus exports, alerting rules,\n   and Grafana dashboards had to be configured by hand\n * This process was required any time a new version of recsys was deployed\n\nAs mentioned before, user tests are must-have in personalization tasks and it's\ncrucial to be able to set up and teardown one as quickly and easily as possible,\notherwise this overhead would have driven our velocity and impact close to zero.\n\nMachine Learning at Scale\nAt this stage, it was clear that a better solution was required. I spent some\ntime researching concepts such as service discovery, canary deployments, and\ncloud-native. Eventually, I discovered Traefik [/traefik/], a nice and easy to\nuse reverse-proxy and load balancer which works natively with Docker containers\nand Docker Swarm.\n\nAfter reading the Traefik documentation [https://docs.traefik.io] I discovered\nthat I would get:\n\n * auto-discovery of new containers\n * health checks and routing only to healthy containers\n * metrics endpoint for Prometheus\n * configuration via Docker labels which means all my infrastructure will be\n   described alongside my deployment manifests, perfect\n * no more exposed ports on my containers, now they are secure and accessible\n   only inside the Docker network\n\nFor a long time, I was curious about the the multi-armed bandit\n[https://en.wikipedia.org/wiki/Multi-armed_bandit] approach for recsys tests.\nThe idea of this approach is to optimize some kind of reward (e.g. user\nsatisfaction) during the constant test with adjustments made on the fly and to\nminimize resources (e.g. user sessions) used during the exploration of new\nchoices (recsys models). It seems perfect if you want to test every small\niteration or even several versions of service at the same time.\n\nI ended up building a couple of different prototypes and came to this solution:\n\nEvery service became a router in Traefik terminology, and each recommender\nengine container became a service [https://docs.traefik.io/routing/services/].\n\nTraefik made it possible to assign multiple Docker services to one routing rule\n[https://docs.traefik.io/routing/routers/] and split traffic between services\nwhile assigning\n[https://docs.traefik.io/routing/services/#weighted-round-robin-service] weights\nto each one. So here is the most interesting part:\n\n * The sum of all weights for different versions of the recsys would always\n   equal 100\n * All new versions were deployed with weight 1, so they received only 1% of\n   user traffic\n * Several times a day Python script recalculated recommender performance\n   metrics for each version of recommender (for simplicity let's say CTR or \n   click-through rate [https://en.wikipedia.org/wiki/Click-through_rate]) and\n   updated weights for models; so better performing ones receive more user\n   requests while others receive less\n\nTesting a new version of the recommender engine is as simple as adding a service\ndefinition to the docker-compose file and updating the stack. Docker Swarm finds\nthe difference between the desired and current state, starts a new Docker\ncontainer with `weight = 1`. Traefik finds this container and, after successful\nhealth checks, routes 1% of requests to the new version, all while exporting the\nmetrics required for monitoring by our automated scripts. After a set amount of\ntime the CTR metrics are recalculated and if users like the new version, it is\nreconfigured to receive a larger share of the traffic, and it is constantly\nbeing reevaluated against new models.\n\nTraefik Helps Me Do What I Love Most\nWe are a small team and I am the only MLE on the project, so I insist on\nresearching and utilizing tools that are easy to use and built with love for the\nend-user. Traefik definitely meets that criteria. Maintenance costs are close to\nzero, and the learning curve is flat. This approach is pretty universal no\nmatter which container orchestrator you use and how you configure Traefik,\nwhether youâ€™re using Kubernetes, Docker labels, Consul, Etcd, or something else.\n\nIn summary, Iâ€™d recommend Traefik [/traefik/] to anyone who needs reverse proxy\nwith all necessary features such as service discovery, monitoring, and metrics\nexporting, which is easy to set up and maintain. It saved me a ton of time so\nthat I could now spend fine-tuning algorithms and adding personal ranking to\neditors' handpicked book sets, which help users find the books they like. All\nthis effort yielded book consumption increase by tens of percent.\n\nAuthor's Bio\nAlex Dmitriev is an engineer in embedded electronics and data scientist in\nmetallurgy. Since 2018, he's been a machine learning engineer at MyBook, and is\nfocused on personalization of user experience and infrastructure for ML\nservices.","html":"<p><strong>Guest post by Alexander Dmitriev, Traefik Ambassador</strong></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://containous.ghost.io/content/images/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik.jpg\" class=\"kg-image\" alt=\"Do Machines Learn? Testing in Production with Traefik\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik.jpg 1000w, https://containous.ghost.io/content/images/size/w1600/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik.jpg 1600w, https://containous.ghost.io/content/images/2020/08/Do-Machines-Learn--Testing-in-Production-with-Traefik.jpg 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure><!--kg-card-begin: markdown--><p>Anyone who runs machine learning models in production while trying to improve model performance most likely knows about <a href=\"https://en.wikipedia.org/wiki/A/B_testing\" target=\"_blank\" rel=\"nofollow\">A/B tests</a>. But what if you are the only machine learning engineer on the project? You must automate these tests as much as possible. And what if you have to make dozens of iterations per year and sometimes deploy new versions several times a week? Here is my story about the personalization of a large-scale book reading service and building the infrastructure to evaluate new models utilizing Traefik.</p>\n<!--kg-card-end: markdown--><h2 id=\"about-mybook\">About MyBook</h2><!--kg-card-begin: markdown--><p><a href=\"https://mybook.ru/\" target=\"_blank\" rel=\"nofollow\">MyBook</a> is a subscription reading service that publishes book apps for multiple platforms while striving to provide the best reading experience for our users. We've collected a huge catalog of books, audiobooks, ratings, and summaries that are conveniently always in your pocket. Bookmarks are synced across all our userâ€™s devices and text-audiobook versions. Avid readers can use a subscription with unlimited reading, and those who want just one book can rent it for half price. My name is Alexander Dmitriev, and I am a machine learning engineer. My field of interest is personalization: <a href=\"https://en.wikipedia.org/wiki/Recommender_system\" target=\"_blank\" rel=\"nofollow\">recommender systems</a>, <a href=\"https://en.wikipedia.org/wiki/Computer_user_satisfaction\" target=\"_blank\" rel=\"nofollow\">user satisfaction measurement</a>, and infrastructure for these systems. I love to solve problems end-to-end: discuss the user interface with the product manager, prepare data, train machine learning models, write the service, deploy to production, and support it.</p>\n<!--kg-card-end: markdown--><h2 id=\"building-a-smart-er-recommendation-engine\">Building a Smart(er) Recommendation Engine</h2><p>Helping customers find books that are interesting to them is essential for a retail service like ours. MyBook already has talented editors curating popular book selections, so the next logical step was adding frequently updated personal recommendations. When I started at MyBook, my goal was to build a recommender system that discovered and suggested books that a customer might find interesting based on their behaviors. This service is known internally as <em>recsys</em>, and I'll be focusing on the evolution, automation, and maturation of that system in this post.</p><p>Production testing is critical for the development of machine learning systems like <em>recsys</em>. Simulating ephemeral properties such as serendipity, variety, and relevance is not feasible with more traditional approaches to testing such as unit or integration methodologies used during software development.</p><p>When the first version of the recommender service was ready, I added a Flask server, built a Docker container, and deployed it on a newly purchased cloud server. Backend engineers added NGINX as a reverse proxy with SSL support because it had been used for many years on our production systems. The <em>recsys </em>application Flask port was published to the host, and all requests were forwarded.</p><p>So, at first, the architecture looked like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_01@2x.jpg\" class=\"kg-image\" alt=\"Architecture with Nginx as reverse proxy\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/06/MyBook_Diagram_01@2x.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/06/MyBook_Diagram_01@2x.jpg 1000w, https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_01@2x.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"the-growing-pains-of-productionized-testing\">The Growing Pains of Productionized Testing</h2><!--kg-card-begin: markdown--><p>We regularly performed A/B tests where we measured the <a href=\"https://en.wikipedia.org/wiki/Mean\" target=\"_blank\" rel=\"nofollow\">mean number</a> of books our users added to their bookshelves during a single session. The end results showed us that personalized sets performed 30% better than a fixed set of bestsellers handpicked by our editors. Eventually, all new versions of recsys rolled out through the A/B test against the previous one in production. And it looked something like this:</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_02@2x.jpg\" class=\"kg-image\" alt=\"Second version of the architecture with Nginx as reverse proxy\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/06/MyBook_Diagram_02@2x.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/06/MyBook_Diagram_02@2x.jpg 1000w, https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_02@2x.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>As time passed and new services appeared, the situation became messy and unmanageable:</p><ul><li>Many of the recsys services ports were hardcoded on the backend</li><li>Prior to each A/B test the health checks, Prometheus exports, alerting rules, and Grafana dashboards had to be configured by hand</li><li>This process was required any time a new version of recsys was deployed</li></ul><p>As mentioned before, user tests are must-have in personalization tasks and it's crucial to be able to set up and teardown one as quickly and easily as possible, otherwise this overhead would have driven our velocity and impact close to zero.</p><h2 id=\"machine-learning-at-scale\">Machine Learning at Scale</h2><p>At this stage, it was clear that a better solution was required. I spent some time researching concepts such as service discovery, canary deployments, and cloud-native. Eventually, I discovered <a href=\"https://containous.ghost.io/traefik/\">Traefik</a>, a nice and easy to use reverse-proxy and load balancer which works natively with Docker containers and Docker Swarm.</p><p>After reading the Traefik <a href=\"https://docs.traefik.io\">documentation</a> I discovered that I would get:</p><ul><li>auto-discovery of new containers</li><li>health checks and routing only to healthy containers</li><li>metrics endpoint for Prometheus</li><li>configuration via Docker labels which means all my infrastructure will be described alongside my deployment manifests, perfect</li><li>no more exposed ports on my containers, now they are secure and accessible only inside the Docker network</li></ul><!--kg-card-begin: markdown--><p>For a long time, I was curious about the <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\" target=\"_blank\" rel=\"nofollow\">the multi-armed bandit</a> approach for recsys tests. The idea of this approach is to optimize some kind of reward (e.g. user satisfaction) during the constant test with adjustments made on the fly and to minimize resources (e.g. user sessions) used during the exploration of new choices (recsys models). It seems perfect if you want to test every small iteration or even several versions of service at the same time.</p>\n<!--kg-card-end: markdown--><p>I ended up building a couple of different prototypes and came to this solution:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_03@2x.jpg\" class=\"kg-image\" alt=\"Architecture with Traefik as reverse proxy\" srcset=\"https://containous.ghost.io/content/images/size/w600/2020/06/MyBook_Diagram_03@2x.jpg 600w, https://containous.ghost.io/content/images/size/w1000/2020/06/MyBook_Diagram_03@2x.jpg 1000w, https://containous.ghost.io/content/images/2020/06/MyBook_Diagram_03@2x.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Every service became a router in Traefik terminology, and each recommender engine container became a <a href=\"https://docs.traefik.io/routing/services/\">service</a>.</p><p>Traefik made it possible to assign multiple Docker services to one <a href=\"https://docs.traefik.io/routing/routers/\">routing rule</a> and split traffic between services while <a href=\"https://docs.traefik.io/routing/services/#weighted-round-robin-service\">assigning</a> weights to each one. So here is the most interesting part:</p><!--kg-card-begin: markdown--><ul>\n<li>The sum of all weights for different versions of the recsys would always equal 100</li>\n<li>All new versions were deployed with weight 1, so they received only 1% of user traffic</li>\n<li>Several times a day Python script recalculated recommender performance metrics for each version of recommender (for simplicity let's say CTR or <a href=\"https://en.wikipedia.org/wiki/Click-through_rate\" target=\"_blank\" rel=\"nofollow\">click-through rate</a>) and updated weights for models; so better performing ones receive more user requests while others receive less</li>\n</ul>\n<!--kg-card-end: markdown--><p>Testing a new version of the recommender engine is as simple as adding a service definition to the docker-compose file and updating the stack. Docker Swarm finds the difference between the desired and current state, starts a new Docker container with `weight = 1`. Traefik finds this container and, after successful health checks, routes 1% of requests to the new version, all while exporting the metrics required for monitoring by our automated scripts. After a set amount of time the CTR metrics are recalculated and if users like the new version, it is reconfigured to receive a larger share of the traffic, and it is constantly being reevaluated against new models.</p><h2 id=\"traefik-helps-me-do-what-i-love-most\">Traefik Helps Me Do What I Love Most</h2><p>We are a small team and I am the only MLE on the project, so I insist on researching and utilizing tools that are easy to use and built with love for the end-user. Traefik definitely meets that criteria. Maintenance costs are close to zero, and the learning curve is flat. This approach is pretty universal no matter which container orchestrator you use and how you configure Traefik, whether youâ€™re using Kubernetes, Docker labels, Consul, Etcd, or something else.</p><p>In summary, Iâ€™d recommend <a href=\"https://containous.ghost.io/traefik/\">Traefik</a> to anyone who needs reverse proxy with all necessary features such as service discovery, monitoring, and metrics exporting, which is easy to set up and maintain. It saved me a ton of time so that I could now spend fine-tuning algorithms and adding personal ranking to editors' handpicked book sets, which help users find the books they like. All this effort yielded book consumption increase by tens of percent.</p><h3 id=\"author-s-bio\">Author's Bio</h3><p>Alex Dmitriev is an engineer in embedded electronics and data scientist in metallurgy. Since 2018, he's been a machine learning engineer at MyBook, and is focused on personalization of user experience and infrastructure for ML services.</p>","url":"https://containous.ghost.io/blog/do-machines-learn-testing-in-production-with-traefik/","canonical_url":null,"uuid":"6fdd3d47-70da-43f9-bdfb-c7f840c91d02","codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5efb70c61555240039b0bf3d","reading_time":5}}]}},"pageContext":{"slug":"datahoarder","limit":9,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}},"staticQueryHashes":["1274566015","2561578252","2731221146","394248586","4145280475","749840385"]}